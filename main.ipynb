{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author__: \"Tan Hua Beng, G2203829L\"\n",
    "\n",
    "__Email__: \"w220032@e.ntu.edus.g\"\n",
    "\n",
    "__Project__ = \"IN6277 Assignment 1.1\"\n",
    "\n",
    "__Description__ = \"Jupyter notebook containing the workings, notes, and code for IN6277 assignment 1.1. This assignment uses a dataset provided and develops a classification model based on a selected algorithm. Apply implemented algorithm to the training dataset, and estimate the performance of the model on the testing dataset.\"\n",
    "\n",
    "__License__ = \"See README.md\"\n",
    "\n",
    "__Version__ = \"1.0.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note: Check the next cell before executing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "# Set to skip visualization cell execution\n",
    "G_SKIP_VISUALIZATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding assignment requirements 1.1\n",
    "    - Implement classification algorithm of choice for prediction\n",
    "    - Prediction task is to determine whether a person makes over 50K a year.\n",
    "\n",
    "2. Initial understanding dataset files, features/ variables, sample space, dimensions, size\n",
    "    ><b>See adult.names file for original description of the dataset.</b>\n",
    "    \n",
    "    - Data source: https://archive.ics.uci.edu/dataset/20/census+income\n",
    "    - Files: adult.data – training data, adult.names - description, adult.text – test data\n",
    "    - 14 features/ attributes/ IVs of categorical/ integer type\n",
    "    - Unknown values represented by \"?\"\n",
    "    - 1 target/ DV Income\n",
    "    - 48842 Instances/ records\n",
    "    - Has missing values\n",
    "    - Extraction conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "    - Variable table as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><thead><tr><th>Variable Name</th><th>Role</th><th>Type</th><th>Demographic</th><th>Description</th><th>Units</th><th>Missing Values</th></tr></thead> <tbody><tr><td>age</td><td>Feature</td><td>Integer</td><td>Age</td><td>N/A</td><td></td><td>no</td> </tr><tr><td>workclass</td><td>Feature</td><td>Categorical</td><td>Income</td><td>Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.</td><td></td><td>yes</td> </tr><tr><td>fnlwgt</td><td>Feature</td><td>Integer</td><td></td><td></td><td></td><td>no</td> </tr><tr><td>education</td><td>Feature</td><td>Categorical</td><td>Education Level</td><td> Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.</td><td></td><td>no</td> </tr><tr><td>education-num</td><td>Feature</td><td>Integer</td><td>Education Level</td><td></td><td></td><td>no</td> </tr><tr><td>marital-status</td><td>Feature</td><td>Categorical</td><td>Other</td><td>Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.</td><td></td><td>no</td> </tr><tr><td>occupation</td><td>Feature</td><td>Categorical</td><td>Other</td><td>Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.</td><td></td><td>yes</td> </tr><tr><td>relationship</td><td>Feature</td><td>Categorical</td><td>Other</td><td>Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.</td><td></td><td>no</td> </tr><tr><td>race</td><td>Feature</td><td>Categorical</td><td>Race</td><td>White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.</td><td></td><td>no</td> </tr><tr><td>sex</td><td>Feature</td><td>Binary</td><td>Sex</td><td>Female, Male.</td><td></td><td>no</td> </tr><tr><td>capital-gain</td><td>Feature</td><td>Integer</td><td></td><td></td><td></td><td>no</td> </tr><tr><td>capital-loss</td><td>Feature</td><td>Integer</td><td></td><td></td><td></td><td>no</td> </tr><tr><td>hours-per-week</td><td>Feature</td><td>Integer</td><td></td><td></td><td></td><td>no</td> </tr><tr><td>native-country</td><td>Feature</td><td>Categorical</td><td>Other</td><td>United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands.</td><td></td><td>yes</td> </tr><tr><td>income</td><td>Target</td><td>Binary</td><td>Income</td><td>&gt;50K, &lt;=50K.</td><td></td><td>no</td> </tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create development environment in anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A virual environment is created with running python version 3.9.18. Following is the requirements.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appnope==0.1.3\n",
      "asttokens==2.4.0\n",
      "backcall==0.2.0\n",
      "backports.functools-lru-cache==1.6.5\n",
      "Bottleneck==1.3.5\n",
      "comm==0.1.4\n",
      "contourpy==1.0.5\n",
      "cycler==0.11.0\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "exceptiongroup==1.1.3\n",
      "executing==1.2.0\n",
      "fonttools==4.25.0\n",
      "importlib-metadata==6.8.0\n",
      "importlib-resources==5.2.0\n",
      "ipykernel==6.25.2\n",
      "ipython==8.15.0\n",
      "jedi==0.19.0\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "jupyter_client==8.3.1\n",
      "jupyter_core==5.3.1\n",
      "kiwisolver==1.4.4\n",
      "MarkupSafe==2.1.1\n",
      "matplotlib==3.7.2\n",
      "matplotlib-inline==0.1.6\n",
      "missingno==0.4.2\n",
      "mkl-fft==1.3.6\n",
      "mkl-random==1.2.2\n",
      "mkl-service==2.4.0\n",
      "munkres==1.1.4\n",
      "nest-asyncio==1.5.6\n",
      "numexpr==2.8.4\n",
      "numpy==1.25.2\n",
      "packaging==23.1\n",
      "pandas==2.0.3\n",
      "parso==0.8.3\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.4.0\n",
      "pip==23.2.1\n",
      "platformdirs==3.10.0\n",
      "prompt-toolkit==3.0.39\n",
      "psutil==5.9.5\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "Pygments==2.16.1\n",
      "pyparsing==3.0.9\n",
      "python-dateutil==2.8.2\n",
      "pytz==2022.7\n",
      "pyzmq==24.0.1\n",
      "scikit-learn==1.2.2\n",
      "scipy==1.11.1\n",
      "seaborn==0.12.2\n",
      "setuptools==68.0.0\n",
      "six==1.16.0\n",
      "stack-data==0.6.2\n",
      "threadpoolctl==2.2.0\n",
      "tornado==6.3.3\n",
      "traitlets==5.10.0\n",
      "typing_extensions==4.7.1\n",
      "tzdata==2023.3\n",
      "wcwidth==0.2.6\n",
      "wheel==0.38.4\n",
      "zipp==3.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip list --format=freeze > requirements.txt\n",
    "!cat requirements.txt\n",
    "!conda list -e > conda_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import jinja2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "G_DATASET_DIR=\"Census_Income_Data_Set\"\n",
    "G_DATASET_FILE_TRAIN=os.path.join(G_DATASET_DIR, \"adult.data\")\n",
    "G_DATASET_FILE_TEST=os.path.join(G_DATASET_DIR, \"adult.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporatory and Pre-processing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the provided dataset for exploratory, dropping redundant features (if required), and handling missing values (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from CSV into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "ds_header_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', \n",
    "                   'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "ds_header_names_encoded = ['age-binned', 'workclass-num', 'fnlwgt', 'education-num', 'marital-status-num', 'occupation-num', 'relationship-num', 'race-num', \n",
    "                   'sex-num', 'capital-gain-binned', 'capital-loss-binned', 'hours-per-week-binned', 'native-country-num', 'income-num']\n",
    "\n",
    "encoded_columns_list = ['occupation', 'education', 'marital-status', 'workclass', 'native-country', 'sex', 'race', 'relationship']\n",
    "\n",
    "encoded_columns_binned_list = ['age', 'hours-per-week', 'capital-gain', 'capital-loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe; dataset has not header; test dataset as a metadata on the first row; unknown values are marked as '?'\n",
    "ds_train = pd.read_csv(G_DATASET_FILE_TRAIN, sep=',', header=None, names=ds_header_names)\n",
    "ds_test = pd.read_csv(G_DATASET_FILE_TEST, sep=',', header=None, names=ds_header_names, skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks on loaded data in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset:  (32561, 15) \n",
      "\n",
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country  income  \n",
      "0          2174             0              40   United-States   <=50K  \n",
      "1             0             0              13   United-States   <=50K  \n",
      "2             0             0              40   United-States   <=50K  \n",
      "3             0             0              40   United-States   <=50K  \n",
      "4             0             0              40            Cuba   <=50K  \n",
      "\n",
      "\n",
      "           age     fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  32561.0    32561.0        32561.0       32561.0       32561.0   \n",
      "mean      38.6   189778.4           10.1        1077.6          87.3   \n",
      "std       13.6   105550.0            2.6        7385.3         403.0   \n",
      "min       17.0    12285.0            1.0           0.0           0.0   \n",
      "25%       28.0   117827.0            9.0           0.0           0.0   \n",
      "50%       37.0   178356.0           10.0           0.0           0.0   \n",
      "75%       48.0   237051.0           12.0           0.0           0.0   \n",
      "max       90.0  1484705.0           16.0       99999.0        4356.0   \n",
      "\n",
      "       hours-per-week  \n",
      "count         32561.0  \n",
      "mean             40.4  \n",
      "std              12.3  \n",
      "min               1.0  \n",
      "25%              40.0  \n",
      "50%              40.0  \n",
      "75%              45.0  \n",
      "max              99.0  \n"
     ]
    }
   ],
   "source": [
    "# sanity checks on train data\n",
    "print(\"Shape of the training dataset: \", str(ds_train.shape), \"\\n\")\n",
    "print(ds_train.head())\n",
    "print(\"\\n\")\n",
    "print(ds_train.describe().round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the testing dataset:  (16281, 15) \n",
      "\n",
      "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n",
      "0   25     Private  226802           11th              7        Never-married   \n",
      "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
      "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
      "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
      "4   18           ?  103497   Some-college             10        Never-married   \n",
      "\n",
      "           occupation relationship    race      sex  capital-gain  \\\n",
      "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
      "1     Farming-fishing      Husband   White     Male             0   \n",
      "2     Protective-serv      Husband   White     Male             0   \n",
      "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
      "4                   ?    Own-child   White   Female             0   \n",
      "\n",
      "   capital-loss  hours-per-week  native-country   income  \n",
      "0             0              40   United-States   <=50K.  \n",
      "1             0              50   United-States   <=50K.  \n",
      "2             0              40   United-States    >50K.  \n",
      "3             0              40   United-States    >50K.  \n",
      "4             0              30   United-States   <=50K.  \n",
      "\n",
      "\n",
      "           age     fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  16281.0    16281.0        16281.0       16281.0       16281.0   \n",
      "mean      38.8   189435.7           10.1        1081.9          87.9   \n",
      "std       13.8   105714.9            2.6        7583.9         403.1   \n",
      "min       17.0    13492.0            1.0           0.0           0.0   \n",
      "25%       28.0   116736.0            9.0           0.0           0.0   \n",
      "50%       37.0   177831.0           10.0           0.0           0.0   \n",
      "75%       48.0   238384.0           12.0           0.0           0.0   \n",
      "max       90.0  1490400.0           16.0       99999.0        3770.0   \n",
      "\n",
      "       hours-per-week  \n",
      "count         16281.0  \n",
      "mean             40.4  \n",
      "std              12.5  \n",
      "min               1.0  \n",
      "25%              40.0  \n",
      "50%              40.0  \n",
      "75%              45.0  \n",
      "max              99.0  \n"
     ]
    }
   ],
   "source": [
    "# sanity checks on test data\n",
    "print(\"Shape of the testing dataset: \", str(ds_test.shape), \"\\n\")\n",
    "print(ds_test.head())\n",
    "print(\"\\n\")\n",
    "print(ds_test.describe().round(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print unique values to explore data values or null in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in training dataset: \n",
      "\n",
      "age :  73   [39 50 38 53 28 37 49 52 31 42 30 23 32 40 34 25 43 54 35 59 56 19 20 45\n",
      " 22 48 21 24 57 44 41 29 18 47 46 36 79 27 67 33 76 17 55 61 70 64 71 68\n",
      " 66 51 58 26 60 90 75 65 77 62 63 80 72 74 69 73 81 78 88 82 83 84 85 86\n",
      " 87] \n",
      "\n",
      "workclass :  9   [' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov'\n",
      " ' ?' ' Self-emp-inc' ' Without-pay' ' Never-worked'] \n",
      "\n",
      "fnlwgt :  21648   [ 77516  83311 215646 ...  34066  84661 257302] \n",
      "\n",
      "education :  16   [' Bachelors' ' HS-grad' ' 11th' ' Masters' ' 9th' ' Some-college'\n",
      " ' Assoc-acdm' ' Assoc-voc' ' 7th-8th' ' Doctorate' ' Prof-school'\n",
      " ' 5th-6th' ' 10th' ' 1st-4th' ' Preschool' ' 12th'] \n",
      "\n",
      "education-num :  16   [13  9  7 14  5 10 12 11  4 16 15  3  6  2  1  8] \n",
      "\n",
      "marital-status :  7   [' Never-married' ' Married-civ-spouse' ' Divorced'\n",
      " ' Married-spouse-absent' ' Separated' ' Married-AF-spouse' ' Widowed'] \n",
      "\n",
      "occupation :  15   [' Adm-clerical' ' Exec-managerial' ' Handlers-cleaners' ' Prof-specialty'\n",
      " ' Other-service' ' Sales' ' Craft-repair' ' Transport-moving'\n",
      " ' Farming-fishing' ' Machine-op-inspct' ' Tech-support' ' ?'\n",
      " ' Protective-serv' ' Armed-Forces' ' Priv-house-serv'] \n",
      "\n",
      "relationship :  6   [' Not-in-family' ' Husband' ' Wife' ' Own-child' ' Unmarried'\n",
      " ' Other-relative'] \n",
      "\n",
      "race :  5   [' White' ' Black' ' Asian-Pac-Islander' ' Amer-Indian-Eskimo' ' Other'] \n",
      "\n",
      "sex :  2   [' Male' ' Female'] \n",
      "\n",
      "capital-gain :  119   [ 2174     0 14084  5178  5013  2407 14344 15024  7688 34095  4064  4386\n",
      "  7298  1409  3674  1055  3464  2050  2176   594 20051  6849  4101  1111\n",
      "  8614  3411  2597 25236  4650  9386  2463  3103 10605  2964  3325  2580\n",
      "  3471  4865 99999  6514  1471  2329  2105  2885 25124 10520  2202  2961\n",
      " 27828  6767  2228  1506 13550  2635  5556  4787  3781  3137  3818  3942\n",
      "   914   401  2829  2977  4934  2062  2354  5455 15020  1424  3273 22040\n",
      "  4416  3908 10566   991  4931  1086  7430  6497   114  7896  2346  3418\n",
      "  3432  2907  1151  2414  2290 15831 41310  4508  2538  3456  6418  1848\n",
      "  3887  5721  9562  1455  2036  1831 11678  2936  2993  7443  6360  1797\n",
      "  1173  4687  6723  2009  6097  2653  1639 18481  7978  2387  5060] \n",
      "\n",
      "capital-loss :  92   [   0 2042 1408 1902 1573 1887 1719 1762 1564 2179 1816 1980 1977 1876\n",
      " 1340 2206 1741 1485 2339 2415 1380 1721 2051 2377 1669 2352 1672  653\n",
      " 2392 1504 2001 1590 1651 1628 1848 1740 2002 1579 2258 1602  419 2547\n",
      " 2174 2205 1726 2444 1138 2238  625  213 1539  880 1668 1092 1594 3004\n",
      " 2231 1844  810 2824 2559 2057 1974  974 2149 1825 1735 1258 2129 2603\n",
      " 2282  323 4356 2246 1617 1648 2489 3770 1755 3683 2267 2080 2457  155\n",
      " 3900 2201 1944 2467 2163 2754 2472 1411] \n",
      "\n",
      "hours-per-week :  94   [40 13 16 45 50 80 30 35 60 20 52 44 15 25 38 43 55 48 58 32 70  2 22 56\n",
      " 41 28 36 24 46 42 12 65  1 10 34 75 98 33 54  8  6 64 19 18 72  5  9 47\n",
      " 37 21 26 14  4 59  7 99 53 39 62 57 78 90 66 11 49 84  3 17 68 27 85 31\n",
      " 51 77 63 23 87 88 73 89 97 94 29 96 67 82 86 91 81 76 92 61 74 95] \n",
      "\n",
      "native-country :  42   [' United-States' ' Cuba' ' Jamaica' ' India' ' ?' ' Mexico' ' South'\n",
      " ' Puerto-Rico' ' Honduras' ' England' ' Canada' ' Germany' ' Iran'\n",
      " ' Philippines' ' Italy' ' Poland' ' Columbia' ' Cambodia' ' Thailand'\n",
      " ' Ecuador' ' Laos' ' Taiwan' ' Haiti' ' Portugal' ' Dominican-Republic'\n",
      " ' El-Salvador' ' France' ' Guatemala' ' China' ' Japan' ' Yugoslavia'\n",
      " ' Peru' ' Outlying-US(Guam-USVI-etc)' ' Scotland' ' Trinadad&Tobago'\n",
      " ' Greece' ' Nicaragua' ' Vietnam' ' Hong' ' Ireland' ' Hungary'\n",
      " ' Holand-Netherlands'] \n",
      "\n",
      "income :  2   [' <=50K' ' >50K'] \n",
      "\n",
      "Unique values in testing dataset: \n",
      "\n",
      "age :  73   [25 38 28 44 18 34 29 63 24 55 65 36 26 58 48 43 20 37 40 72 45 22 23 54\n",
      " 32 46 56 17 39 52 21 42 33 30 47 41 19 69 50 31 59 49 51 27 57 61 64 79\n",
      " 73 53 77 80 62 35 68 66 75 60 67 71 70 90 81 74 78 82 83 85 76 84 89 88\n",
      " 87] \n",
      "\n",
      "workclass :  9   [' Private' ' Local-gov' ' ?' ' Self-emp-not-inc' ' Federal-gov'\n",
      " ' State-gov' ' Self-emp-inc' ' Without-pay' ' Never-worked'] \n",
      "\n",
      "fnlwgt :  12787   [226802  89814 336951 ... 349230 321403  83891] \n",
      "\n",
      "education :  16   [' 11th' ' HS-grad' ' Assoc-acdm' ' Some-college' ' 10th' ' Prof-school'\n",
      " ' 7th-8th' ' Bachelors' ' Masters' ' Doctorate' ' 5th-6th' ' Assoc-voc'\n",
      " ' 9th' ' 12th' ' 1st-4th' ' Preschool'] \n",
      "\n",
      "education-num :  16   [ 7  9 12 10  6 15  4 13 14 16  3 11  5  8  2  1] \n",
      "\n",
      "marital-status :  7   [' Never-married' ' Married-civ-spouse' ' Widowed' ' Divorced'\n",
      " ' Separated' ' Married-spouse-absent' ' Married-AF-spouse'] \n",
      "\n",
      "occupation :  15   [' Machine-op-inspct' ' Farming-fishing' ' Protective-serv' ' ?'\n",
      " ' Other-service' ' Prof-specialty' ' Craft-repair' ' Adm-clerical'\n",
      " ' Exec-managerial' ' Tech-support' ' Sales' ' Priv-house-serv'\n",
      " ' Transport-moving' ' Handlers-cleaners' ' Armed-Forces'] \n",
      "\n",
      "relationship :  6   [' Own-child' ' Husband' ' Not-in-family' ' Unmarried' ' Wife'\n",
      " ' Other-relative'] \n",
      "\n",
      "race :  5   [' Black' ' White' ' Asian-Pac-Islander' ' Other' ' Amer-Indian-Eskimo'] \n",
      "\n",
      "sex :  2   [' Male' ' Female'] \n",
      "\n",
      "capital-gain :  113   [    0  7688  3103  6418  7298  3908 14084  5178 15024 99999  2597  2907\n",
      "  4650  6497  1055  5013 27828  4934  4064  3674  2174 10605  3418   114\n",
      "  2580  3411  4508  4386  8614 13550  6849  2463  3137  2885  2964  1471\n",
      " 10566  2354  1424  1455  3325  4416 25236   594  2105  4787  2829   401\n",
      "  4865  1264  1506 10520  3464  2653 20051  4101  1797  2407  3471  1086\n",
      "  1848 14344  1151  2993  2290 15020  9386  2202  3818  2176  5455 11678\n",
      "  7978  7262  6514 41310  3456  7430  2414  2062 34095  1831  6723  5060\n",
      " 15831  2977  2346  3273  2329  9562  2635  4931  1731  6097   914  7896\n",
      "  5556  1409  3781  3942  2538  3887 25124  7443  5721  1173  4687  6612\n",
      "  6767  2961   991  2036  2936] \n",
      "\n",
      "capital-loss :  82   [   0 1721 1876 2415 1887  625 1977 2057 1429 1590 1485 2051 2377 1672\n",
      " 1628 1902 1602 1741 2444 1408 2001 2042 1740 1825 1848 1719 3004 2179\n",
      " 1573 2205 1258 2339 1726 2258 1340 1504 2559 1668 1974 1980 1564 2547\n",
      " 2002 1669 1617  323 3175 2472 2174 1579 2129 1510 1735 2282 1870 1411\n",
      " 1911 1651 1092 1762 2457 2231 2238  653 1138 2246 2603 2392 1944 1380\n",
      " 2465 1421 3770 1594  213 2149 2824 1844 2467 2163 1816 1648] \n",
      "\n",
      "hours-per-week :  89   [40 50 30 32 10 39 35 48 25 20 45 47  6 43 90 54 60 38 36 18 24 44 56 28\n",
      " 16 41 22 55 14 33 37  8 12 70 15 75 52 84 42 80 68 99 65  5 17 72 53 29\n",
      " 96 21 46  3  1 23 49 67 76  7  2 58 26 34  4 51 78 63 31 92 77 27 85 13\n",
      " 19 98 62 66 57 11 86 59  9 64 73 61 88 79 89 74 69] \n",
      "\n",
      "native-country :  41   [' United-States' ' ?' ' Peru' ' Guatemala' ' Mexico'\n",
      " ' Dominican-Republic' ' Ireland' ' Germany' ' Philippines' ' Thailand'\n",
      " ' Haiti' ' El-Salvador' ' Puerto-Rico' ' Vietnam' ' South' ' Columbia'\n",
      " ' Japan' ' India' ' Cambodia' ' Poland' ' Laos' ' England' ' Cuba'\n",
      " ' Taiwan' ' Italy' ' Canada' ' Portugal' ' China' ' Nicaragua'\n",
      " ' Honduras' ' Iran' ' Scotland' ' Jamaica' ' Ecuador' ' Yugoslavia'\n",
      " ' Hungary' ' Hong' ' Greece' ' Trinadad&Tobago'\n",
      " ' Outlying-US(Guam-USVI-etc)' ' France'] \n",
      "\n",
      "income :  2   [' <=50K.' ' >50K.'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education-num     0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to print unique values of each column in loaded dataset\n",
    "def print_unique_values(dataset, dataset_name = \"dataset\"):\n",
    "    print(f\"Unique values in {dataset_name}: \\n\")\n",
    "    for col in dataset.columns:\n",
    "        print(col, \": \", dataset[col].nunique(), \" \", dataset[col].unique(), \"\\n\")\n",
    "\n",
    "# Check and print unique values of each column\n",
    "print_unique_values(ds_train, \"training dataset\")\n",
    "print_unique_values(ds_test, \"testing dataset\")\n",
    "\n",
    "ds_train.isnull().sum()\n",
    "ds_test.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim whitespaces in data values noticed from exploring unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in training dataset: \n",
      "\n",
      "age :  73   [39 50 38 53 28 37 49 52 31 42 30 23 32 40 34 25 43 54 35 59 56 19 20 45\n",
      " 22 48 21 24 57 44 41 29 18 47 46 36 79 27 67 33 76 17 55 61 70 64 71 68\n",
      " 66 51 58 26 60 90 75 65 77 62 63 80 72 74 69 73 81 78 88 82 83 84 85 86\n",
      " 87] \n",
      "\n",
      "workclass :  9   ['State-gov' 'Self-emp-not-inc' 'Private' 'Federal-gov' 'Local-gov' '?'\n",
      " 'Self-emp-inc' 'Without-pay' 'Never-worked'] \n",
      "\n",
      "fnlwgt :  21648   [ 77516  83311 215646 ...  34066  84661 257302] \n",
      "\n",
      "education :  16   ['Bachelors' 'HS-grad' '11th' 'Masters' '9th' 'Some-college' 'Assoc-acdm'\n",
      " 'Assoc-voc' '7th-8th' 'Doctorate' 'Prof-school' '5th-6th' '10th'\n",
      " '1st-4th' 'Preschool' '12th'] \n",
      "\n",
      "education-num :  16   [13  9  7 14  5 10 12 11  4 16 15  3  6  2  1  8] \n",
      "\n",
      "marital-status :  7   ['Never-married' 'Married-civ-spouse' 'Divorced' 'Married-spouse-absent'\n",
      " 'Separated' 'Married-AF-spouse' 'Widowed'] \n",
      "\n",
      "occupation :  15   ['Adm-clerical' 'Exec-managerial' 'Handlers-cleaners' 'Prof-specialty'\n",
      " 'Other-service' 'Sales' 'Craft-repair' 'Transport-moving'\n",
      " 'Farming-fishing' 'Machine-op-inspct' 'Tech-support' '?'\n",
      " 'Protective-serv' 'Armed-Forces' 'Priv-house-serv'] \n",
      "\n",
      "relationship :  6   ['Not-in-family' 'Husband' 'Wife' 'Own-child' 'Unmarried' 'Other-relative'] \n",
      "\n",
      "race :  5   ['White' 'Black' 'Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Other'] \n",
      "\n",
      "sex :  2   ['Male' 'Female'] \n",
      "\n",
      "capital-gain :  119   [ 2174     0 14084  5178  5013  2407 14344 15024  7688 34095  4064  4386\n",
      "  7298  1409  3674  1055  3464  2050  2176   594 20051  6849  4101  1111\n",
      "  8614  3411  2597 25236  4650  9386  2463  3103 10605  2964  3325  2580\n",
      "  3471  4865 99999  6514  1471  2329  2105  2885 25124 10520  2202  2961\n",
      " 27828  6767  2228  1506 13550  2635  5556  4787  3781  3137  3818  3942\n",
      "   914   401  2829  2977  4934  2062  2354  5455 15020  1424  3273 22040\n",
      "  4416  3908 10566   991  4931  1086  7430  6497   114  7896  2346  3418\n",
      "  3432  2907  1151  2414  2290 15831 41310  4508  2538  3456  6418  1848\n",
      "  3887  5721  9562  1455  2036  1831 11678  2936  2993  7443  6360  1797\n",
      "  1173  4687  6723  2009  6097  2653  1639 18481  7978  2387  5060] \n",
      "\n",
      "capital-loss :  92   [   0 2042 1408 1902 1573 1887 1719 1762 1564 2179 1816 1980 1977 1876\n",
      " 1340 2206 1741 1485 2339 2415 1380 1721 2051 2377 1669 2352 1672  653\n",
      " 2392 1504 2001 1590 1651 1628 1848 1740 2002 1579 2258 1602  419 2547\n",
      " 2174 2205 1726 2444 1138 2238  625  213 1539  880 1668 1092 1594 3004\n",
      " 2231 1844  810 2824 2559 2057 1974  974 2149 1825 1735 1258 2129 2603\n",
      " 2282  323 4356 2246 1617 1648 2489 3770 1755 3683 2267 2080 2457  155\n",
      " 3900 2201 1944 2467 2163 2754 2472 1411] \n",
      "\n",
      "hours-per-week :  94   [40 13 16 45 50 80 30 35 60 20 52 44 15 25 38 43 55 48 58 32 70  2 22 56\n",
      " 41 28 36 24 46 42 12 65  1 10 34 75 98 33 54  8  6 64 19 18 72  5  9 47\n",
      " 37 21 26 14  4 59  7 99 53 39 62 57 78 90 66 11 49 84  3 17 68 27 85 31\n",
      " 51 77 63 23 87 88 73 89 97 94 29 96 67 82 86 91 81 76 92 61 74 95] \n",
      "\n",
      "native-country :  42   ['United-States' 'Cuba' 'Jamaica' 'India' '?' 'Mexico' 'South'\n",
      " 'Puerto-Rico' 'Honduras' 'England' 'Canada' 'Germany' 'Iran'\n",
      " 'Philippines' 'Italy' 'Poland' 'Columbia' 'Cambodia' 'Thailand' 'Ecuador'\n",
      " 'Laos' 'Taiwan' 'Haiti' 'Portugal' 'Dominican-Republic' 'El-Salvador'\n",
      " 'France' 'Guatemala' 'China' 'Japan' 'Yugoslavia' 'Peru'\n",
      " 'Outlying-US(Guam-USVI-etc)' 'Scotland' 'Trinadad&Tobago' 'Greece'\n",
      " 'Nicaragua' 'Vietnam' 'Hong' 'Ireland' 'Hungary' 'Holand-Netherlands'] \n",
      "\n",
      "income :  2   ['<=50K' '>50K'] \n",
      "\n",
      "Unique values in testing dataset: \n",
      "\n",
      "age :  73   [25 38 28 44 18 34 29 63 24 55 65 36 26 58 48 43 20 37 40 72 45 22 23 54\n",
      " 32 46 56 17 39 52 21 42 33 30 47 41 19 69 50 31 59 49 51 27 57 61 64 79\n",
      " 73 53 77 80 62 35 68 66 75 60 67 71 70 90 81 74 78 82 83 85 76 84 89 88\n",
      " 87] \n",
      "\n",
      "workclass :  9   ['Private' 'Local-gov' '?' 'Self-emp-not-inc' 'Federal-gov' 'State-gov'\n",
      " 'Self-emp-inc' 'Without-pay' 'Never-worked'] \n",
      "\n",
      "fnlwgt :  12787   [226802  89814 336951 ... 349230 321403  83891] \n",
      "\n",
      "education :  16   ['11th' 'HS-grad' 'Assoc-acdm' 'Some-college' '10th' 'Prof-school'\n",
      " '7th-8th' 'Bachelors' 'Masters' 'Doctorate' '5th-6th' 'Assoc-voc' '9th'\n",
      " '12th' '1st-4th' 'Preschool'] \n",
      "\n",
      "education-num :  16   [ 7  9 12 10  6 15  4 13 14 16  3 11  5  8  2  1] \n",
      "\n",
      "marital-status :  7   ['Never-married' 'Married-civ-spouse' 'Widowed' 'Divorced' 'Separated'\n",
      " 'Married-spouse-absent' 'Married-AF-spouse'] \n",
      "\n",
      "occupation :  15   ['Machine-op-inspct' 'Farming-fishing' 'Protective-serv' '?'\n",
      " 'Other-service' 'Prof-specialty' 'Craft-repair' 'Adm-clerical'\n",
      " 'Exec-managerial' 'Tech-support' 'Sales' 'Priv-house-serv'\n",
      " 'Transport-moving' 'Handlers-cleaners' 'Armed-Forces'] \n",
      "\n",
      "relationship :  6   ['Own-child' 'Husband' 'Not-in-family' 'Unmarried' 'Wife' 'Other-relative'] \n",
      "\n",
      "race :  5   ['Black' 'White' 'Asian-Pac-Islander' 'Other' 'Amer-Indian-Eskimo'] \n",
      "\n",
      "sex :  2   ['Male' 'Female'] \n",
      "\n",
      "capital-gain :  113   [    0  7688  3103  6418  7298  3908 14084  5178 15024 99999  2597  2907\n",
      "  4650  6497  1055  5013 27828  4934  4064  3674  2174 10605  3418   114\n",
      "  2580  3411  4508  4386  8614 13550  6849  2463  3137  2885  2964  1471\n",
      " 10566  2354  1424  1455  3325  4416 25236   594  2105  4787  2829   401\n",
      "  4865  1264  1506 10520  3464  2653 20051  4101  1797  2407  3471  1086\n",
      "  1848 14344  1151  2993  2290 15020  9386  2202  3818  2176  5455 11678\n",
      "  7978  7262  6514 41310  3456  7430  2414  2062 34095  1831  6723  5060\n",
      " 15831  2977  2346  3273  2329  9562  2635  4931  1731  6097   914  7896\n",
      "  5556  1409  3781  3942  2538  3887 25124  7443  5721  1173  4687  6612\n",
      "  6767  2961   991  2036  2936] \n",
      "\n",
      "capital-loss :  82   [   0 1721 1876 2415 1887  625 1977 2057 1429 1590 1485 2051 2377 1672\n",
      " 1628 1902 1602 1741 2444 1408 2001 2042 1740 1825 1848 1719 3004 2179\n",
      " 1573 2205 1258 2339 1726 2258 1340 1504 2559 1668 1974 1980 1564 2547\n",
      " 2002 1669 1617  323 3175 2472 2174 1579 2129 1510 1735 2282 1870 1411\n",
      " 1911 1651 1092 1762 2457 2231 2238  653 1138 2246 2603 2392 1944 1380\n",
      " 2465 1421 3770 1594  213 2149 2824 1844 2467 2163 1816 1648] \n",
      "\n",
      "hours-per-week :  89   [40 50 30 32 10 39 35 48 25 20 45 47  6 43 90 54 60 38 36 18 24 44 56 28\n",
      " 16 41 22 55 14 33 37  8 12 70 15 75 52 84 42 80 68 99 65  5 17 72 53 29\n",
      " 96 21 46  3  1 23 49 67 76  7  2 58 26 34  4 51 78 63 31 92 77 27 85 13\n",
      " 19 98 62 66 57 11 86 59  9 64 73 61 88 79 89 74 69] \n",
      "\n",
      "native-country :  41   ['United-States' '?' 'Peru' 'Guatemala' 'Mexico' 'Dominican-Republic'\n",
      " 'Ireland' 'Germany' 'Philippines' 'Thailand' 'Haiti' 'El-Salvador'\n",
      " 'Puerto-Rico' 'Vietnam' 'South' 'Columbia' 'Japan' 'India' 'Cambodia'\n",
      " 'Poland' 'Laos' 'England' 'Cuba' 'Taiwan' 'Italy' 'Canada' 'Portugal'\n",
      " 'China' 'Nicaragua' 'Honduras' 'Iran' 'Scotland' 'Jamaica' 'Ecuador'\n",
      " 'Yugoslavia' 'Hungary' 'Hong' 'Greece' 'Trinadad&Tobago'\n",
      " 'Outlying-US(Guam-USVI-etc)' 'France'] \n",
      "\n",
      "income :  2   ['<=50K.' '>50K.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trim whitespace from each value in each column\n",
    "ds_train = ds_train.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "ds_test = ds_test.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Check and print unique values of each column after trimming\n",
    "print_unique_values(ds_train, \"training dataset\")\n",
    "print_unique_values(ds_test, \"testing dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove weird period in test dataset income colume - '<=50K.' '>50K.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset income column unique values:  2 ['<=50K' '>50K']\n",
      "Testing dataset income column unique values:  2 ['<=50K' '>50K']\n"
     ]
    }
   ],
   "source": [
    "# Remove trailing periods in the income column\n",
    "ds_train['income'] = ds_train['income'].str.replace('.', '')\n",
    "ds_test['income'] = ds_test['income'].str.replace('.', '')\n",
    "\n",
    "# Check and print unique values of each column after removing trailing periods\n",
    "print(\"Training dataset income column unique values: \", ds_train['income'].nunique(), ds_train['income'].unique())\n",
    "print(\"Testing dataset income column unique values: \", ds_test['income'].nunique(), ds_test['income'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "# Function to receive training and testing datasets and return a single dataset\n",
    "def get_dataset(ds_train, ds_test):\n",
    "    ds_train_copy = ds_train.copy()\n",
    "    ds_test_copy = ds_test.copy()\n",
    "    ds_train_copy['dataset'] = 'train'\n",
    "    ds_test_copy['dataset'] = 'test'\n",
    "    ds = pd.concat([ds_train_copy, ds_test_copy], ignore_index=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined dataset:  (48842, 16) \n",
      "\n",
      "   age         workclass  fnlwgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country income dataset  \n",
      "0          2174             0              40  United-States  <=50K   train  \n",
      "1             0             0              13  United-States  <=50K   train  \n",
      "2             0             0              40  United-States  <=50K   train  \n",
      "3             0             0              40  United-States  <=50K   train  \n",
      "4             0             0              40           Cuba  <=50K   train  \n"
     ]
    }
   ],
   "source": [
    "# Plot combined train and test data for visualization\n",
    "# Concat ds_train and ds_test\n",
    "ds_combined = get_dataset(ds_train, ds_test)\n",
    "\n",
    "# Print shape and data of ds_combined\n",
    "print(\"Shape of the combined dataset: \", str(ds_combined.shape), \"\\n\")\n",
    "print(ds_combined.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data visually to check distribution and split\n",
    "A quick comparison of the data distribution similarity, split, and correlation between variables of the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plat combined data for visualization with alpha=0.1 with tight_layout=True, to observe if test and train data are similar in distribution/ representation\n",
    "    sns.pairplot(ds_combined, hue='dataset', plot_kws={'alpha': 0.1}, palette='husl', markers=['x', '+'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encode non-numerical categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in occupation-num column:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute ds_combined occupation column with numerical encoding\n",
    "ds_combined['occupation-num'] = ds_combined['occupation'].replace(['?','Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial', 'Farming-fishing',\n",
    "                                                                   'Handlers-cleaners', 'Machine-op-inspct', 'Other-service', 'Priv-house-serv', 'Prof-specialty',\n",
    "                                                                   'Protective-serv', 'Sales', 'Tech-support', 'Transport-moving'], [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14])\n",
    "\n",
    "# Check if ds_combined has non-numeric values in occupation column, print unique values of occupation-num column\n",
    "temp_list = ds_combined['occupation-num'].unique().tolist()\n",
    "\n",
    "# Order values and print unique values of occupation-num column\n",
    "temp_list.sort()\n",
    "print(\"Unique values in occupation-num column: \", temp_list, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert income column to 0 and 1\n",
    "ds_combined['income-num'] = ds_combined['income'].replace(['<=50K', '>50K'], [0,1])\n",
    "\n",
    "# Convert sex column to 0 and 1\n",
    "ds_combined['sex-num'] = ds_combined['sex'].replace(['Male', 'Female'], [0,1])\n",
    "\n",
    "# Convert race column to numerical values\n",
    "ds_combined['race-num'] = ds_combined['race'].replace(['Black', 'White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'], [0,1,2,3,4])\n",
    "\n",
    "# Convert marital-status column to 0 and 1\n",
    "ds_combined['marital-status-num'] = ds_combined['marital-status'].replace(['Never-married', 'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "                                                                           'Separated', 'Married-AF-spouse', 'Widowed'], [0,1,2,3,4,5,6])\n",
    "\n",
    "# Convert workclass column to numerical values\n",
    "ds_combined['workclass-num'] = ds_combined['workclass'].replace(['?', 'Private', 'Self-emp-not-inc', 'Local-gov', 'State-gov', 'Self-emp-inc', 'Federal-gov',\n",
    "                                                                 'Without-pay', 'Never-worked'], [0,1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert native-country column to numerical values\n",
    "ds_combined['native-country-num'] = ds_combined['native-country'].replace(['?', 'United-States', 'Mexico', 'Philippines', 'Germany', 'Canada', 'Puerto-Rico',\n",
    "                                                                           'El-Salvador', 'India', 'Cuba', 'England', 'Jamaica', 'South', 'China', 'Italy',\n",
    "                                                                           'Dominican-Republic', 'Vietnam', 'Guatemala', 'Japan', 'Poland', 'Columbia',\n",
    "                                                                           'Taiwan', 'Haiti', 'Iran', 'Portugal', 'Nicaragua', 'Peru', 'Greece', 'France',\n",
    "                                                                           'Ecuador', 'Ireland', 'Hong', 'Cambodia', 'Trinadad&Tobago', 'Thailand', 'Laos',\n",
    "                                                                           'Yugoslavia', 'Outlying-US(Guam-USVI-etc)', 'Hungary', 'Honduras', 'Scotland',\n",
    "                                                                           'Holand-Netherlands'], [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n",
    "                                                                                                   21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41])\n",
    "\n",
    "# Convert relationship column to numerical values\n",
    "ds_combined['relationship-num'] = ds_combined['relationship'].replace(['Not-in-family', 'Husband', 'Own-child', 'Unmarried', 'Wife', 'Other-relative'], [0,1,2,3,4,5])\n",
    "\n",
    "# Convert age column into bins\n",
    "ds_combined['age-binned'] = pd.cut(ds_combined['age'], bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Convert working hours per week into bins\n",
    "ds_combined['hours-per-week-binned'] = pd.cut(ds_combined['hours-per-week'], bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Convert capital-gain column into bins, starting with -1 to include 0 values\n",
    "ds_combined['capital-gain-binned'] = pd.cut(ds_combined['capital-gain'], bins=[-1, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000], labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Convert capital-loss column into bins, starting with -1 to include 0 values\n",
    "ds_combined['capital-loss-binned'] = pd.cut(ds_combined['capital-loss'], bins=[-1, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000], labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in combined dataset: \n",
      "\n",
      "age :  74   [39 50 38 53 28 37 49 52 31 42 30 23 32 40 34 25 43 54 35 59 56 19 20 45\n",
      " 22 48 21 24 57 44 41 29 18 47 46 36 79 27 67 33 76 17 55 61 70 64 71 68\n",
      " 66 51 58 26 60 90 75 65 77 62 63 80 72 74 69 73 81 78 88 82 83 84 85 86\n",
      " 87 89] \n",
      "\n",
      "workclass :  9   ['State-gov' 'Self-emp-not-inc' 'Private' 'Federal-gov' 'Local-gov' '?'\n",
      " 'Self-emp-inc' 'Without-pay' 'Never-worked'] \n",
      "\n",
      "fnlwgt :  28523   [ 77516  83311 215646 ... 173449  89686 350977] \n",
      "\n",
      "education :  16   ['Bachelors' 'HS-grad' '11th' 'Masters' '9th' 'Some-college' 'Assoc-acdm'\n",
      " 'Assoc-voc' '7th-8th' 'Doctorate' 'Prof-school' '5th-6th' '10th'\n",
      " '1st-4th' 'Preschool' '12th'] \n",
      "\n",
      "education-num :  16   [13  9  7 14  5 10 12 11  4 16 15  3  6  2  1  8] \n",
      "\n",
      "marital-status :  7   ['Never-married' 'Married-civ-spouse' 'Divorced' 'Married-spouse-absent'\n",
      " 'Separated' 'Married-AF-spouse' 'Widowed'] \n",
      "\n",
      "occupation :  15   ['Adm-clerical' 'Exec-managerial' 'Handlers-cleaners' 'Prof-specialty'\n",
      " 'Other-service' 'Sales' 'Craft-repair' 'Transport-moving'\n",
      " 'Farming-fishing' 'Machine-op-inspct' 'Tech-support' '?'\n",
      " 'Protective-serv' 'Armed-Forces' 'Priv-house-serv'] \n",
      "\n",
      "relationship :  6   ['Not-in-family' 'Husband' 'Wife' 'Own-child' 'Unmarried' 'Other-relative'] \n",
      "\n",
      "race :  5   ['White' 'Black' 'Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Other'] \n",
      "\n",
      "sex :  2   ['Male' 'Female'] \n",
      "\n",
      "capital-gain :  123   [ 2174     0 14084  5178  5013  2407 14344 15024  7688 34095  4064  4386\n",
      "  7298  1409  3674  1055  3464  2050  2176   594 20051  6849  4101  1111\n",
      "  8614  3411  2597 25236  4650  9386  2463  3103 10605  2964  3325  2580\n",
      "  3471  4865 99999  6514  1471  2329  2105  2885 25124 10520  2202  2961\n",
      " 27828  6767  2228  1506 13550  2635  5556  4787  3781  3137  3818  3942\n",
      "   914   401  2829  2977  4934  2062  2354  5455 15020  1424  3273 22040\n",
      "  4416  3908 10566   991  4931  1086  7430  6497   114  7896  2346  3418\n",
      "  3432  2907  1151  2414  2290 15831 41310  4508  2538  3456  6418  1848\n",
      "  3887  5721  9562  1455  2036  1831 11678  2936  2993  7443  6360  1797\n",
      "  1173  4687  6723  2009  6097  2653  1639 18481  7978  2387  5060  1264\n",
      "  7262  1731  6612] \n",
      "\n",
      "capital-loss :  99   [   0 2042 1408 1902 1573 1887 1719 1762 1564 2179 1816 1980 1977 1876\n",
      " 1340 2206 1741 1485 2339 2415 1380 1721 2051 2377 1669 2352 1672  653\n",
      " 2392 1504 2001 1590 1651 1628 1848 1740 2002 1579 2258 1602  419 2547\n",
      " 2174 2205 1726 2444 1138 2238  625  213 1539  880 1668 1092 1594 3004\n",
      " 2231 1844  810 2824 2559 2057 1974  974 2149 1825 1735 1258 2129 2603\n",
      " 2282  323 4356 2246 1617 1648 2489 3770 1755 3683 2267 2080 2457  155\n",
      " 3900 2201 1944 2467 2163 2754 2472 1411 1429 3175 1510 1870 1911 2465\n",
      " 1421] \n",
      "\n",
      "hours-per-week :  96   [40 13 16 45 50 80 30 35 60 20 52 44 15 25 38 43 55 48 58 32 70  2 22 56\n",
      " 41 28 36 24 46 42 12 65  1 10 34 75 98 33 54  8  6 64 19 18 72  5  9 47\n",
      " 37 21 26 14  4 59  7 99 53 39 62 57 78 90 66 11 49 84  3 17 68 27 85 31\n",
      " 51 77 63 23 87 88 73 89 97 94 29 96 67 82 86 91 81 76 92 61 74 95 79 69] \n",
      "\n",
      "native-country :  42   ['United-States' 'Cuba' 'Jamaica' 'India' '?' 'Mexico' 'South'\n",
      " 'Puerto-Rico' 'Honduras' 'England' 'Canada' 'Germany' 'Iran'\n",
      " 'Philippines' 'Italy' 'Poland' 'Columbia' 'Cambodia' 'Thailand' 'Ecuador'\n",
      " 'Laos' 'Taiwan' 'Haiti' 'Portugal' 'Dominican-Republic' 'El-Salvador'\n",
      " 'France' 'Guatemala' 'China' 'Japan' 'Yugoslavia' 'Peru'\n",
      " 'Outlying-US(Guam-USVI-etc)' 'Scotland' 'Trinadad&Tobago' 'Greece'\n",
      " 'Nicaragua' 'Vietnam' 'Hong' 'Ireland' 'Hungary' 'Holand-Netherlands'] \n",
      "\n",
      "income :  2   ['<=50K' '>50K'] \n",
      "\n",
      "dataset :  2   ['train' 'test'] \n",
      "\n",
      "occupation-num :  15   [ 1  4  6 10  8 12  3 14  5  7 13  0 11  2  9] \n",
      "\n",
      "income-num :  2   [0 1] \n",
      "\n",
      "sex-num :  2   [0 1] \n",
      "\n",
      "race-num :  5   [1 0 2 3 4] \n",
      "\n",
      "marital-status-num :  7   [0 1 2 3 4 5 6] \n",
      "\n",
      "workclass-num :  9   [4 2 1 6 3 0 5 7 8] \n",
      "\n",
      "native-country-num :  42   [ 1  9 11  8  0  2 12  6 39 10  5  4 23  3 14 19 20 32 34 29 35 21 22 24\n",
      " 15  7 28 17 13 18 36 26 37 40 33 27 25 16 31 30 38 41] \n",
      "\n",
      "relationship-num :  6   [0 1 4 2 3 5] \n",
      "\n",
      "age-binned :  8   [3, 4, 5, 2, 1, 7, 6, 8]\n",
      "Categories (10, int64): [0 < 1 < 2 < 3 ... 6 < 7 < 8 < 9] \n",
      "\n",
      "hours-per-week-binned :  10   [3, 1, 4, 7, 2, 5, 6, 0, 9, 8]\n",
      "Categories (10, int64): [0 < 1 < 2 < 3 ... 6 < 7 < 8 < 9] \n",
      "\n",
      "capital-gain-binned :  6   [0, 1, 3, 2, 9, 4]\n",
      "Categories (10, int64): [0 < 1 < 2 < 3 ... 6 < 7 < 8 < 9] \n",
      "\n",
      "capital-loss-binned :  5   [0, 2, 1, 3, 4]\n",
      "Categories (10, int64): [0 < 1 < 2 < 3 ... 6 < 7 < 8 < 9] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age                      0\n",
       "workclass                0\n",
       "fnlwgt                   0\n",
       "education                0\n",
       "education-num            0\n",
       "marital-status           0\n",
       "occupation               0\n",
       "relationship             0\n",
       "race                     0\n",
       "sex                      0\n",
       "capital-gain             0\n",
       "capital-loss             0\n",
       "hours-per-week           0\n",
       "native-country           0\n",
       "income                   0\n",
       "dataset                  0\n",
       "occupation-num           0\n",
       "income-num               0\n",
       "sex-num                  0\n",
       "race-num                 0\n",
       "marital-status-num       0\n",
       "workclass-num            0\n",
       "native-country-num       0\n",
       "relationship-num         0\n",
       "age-binned               0\n",
       "hours-per-week-binned    0\n",
       "capital-gain-binned      0\n",
       "capital-loss-binned      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if ds_combined has unexpected values in occupation-num, income-num, sex-num, race-num, marital-status-num, workclass-num, native-country-num, relationship-num, hours-per-week-binned columns\n",
    "print_unique_values(ds_combined, \"combined dataset\")\n",
    "\n",
    "# Check if any columns have null values\n",
    "ds_combined.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Histograms for visualization (Unstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plot ds_combined_high_income and ds_combined_low_income data for all columns stacked vertically\n",
    "    columns_to_plot = ds_header_names_encoded\n",
    "\n",
    "    # Set the style for the plots (optional)\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Create subplots for each column\n",
    "    for column in columns_to_plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Create the histograms using Seaborn's histplot\n",
    "        sns.histplot(ds_combined[ds_combined['income'] == '>50K'][column], kde=True, color='blue', label='>50K')\n",
    "        sns.histplot(ds_combined[ds_combined['income'] == '<=50K'][column], kde=True, color='red', label='<=50K')\n",
    "        \n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.title(f'Histogram of {column} by Income Group')\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graph to visualize data using occupation-num and education-num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plot graph with occupation-num column and education-num column\n",
    "    sns.violinplot(x='occupation-num', y='education-num', data=ds_combined, palette='husl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot stacked crosstab by income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding of occupation column mapping: \n",
      "\n",
      "           occupation  occupation-num  counts\n",
      "0                   ?               0    2809\n",
      "1        Adm-clerical               1    5611\n",
      "2        Armed-Forces               2      15\n",
      "3        Craft-repair               3    6112\n",
      "4     Exec-managerial               4    6086\n",
      "5     Farming-fishing               5    1490\n",
      "6   Handlers-cleaners               6    2072\n",
      "7   Machine-op-inspct               7    3022\n",
      "8       Other-service               8    4923\n",
      "9     Priv-house-serv               9     242\n",
      "10     Prof-specialty              10    6172\n",
      "11    Protective-serv              11     983\n",
      "12              Sales              12    5504\n",
      "13       Tech-support              13    1446\n",
      "14   Transport-moving              14    2355\n",
      "\n",
      "\n",
      "Encoding of education column mapping: \n",
      "\n",
      "       education  education-num  counts\n",
      "0           10th              6    1389\n",
      "1           11th              7    1812\n",
      "2           12th              8     657\n",
      "3        1st-4th              2     247\n",
      "4        5th-6th              3     509\n",
      "5        7th-8th              4     955\n",
      "6            9th              5     756\n",
      "7     Assoc-acdm             12    1601\n",
      "8      Assoc-voc             11    2061\n",
      "9      Bachelors             13    8025\n",
      "10     Doctorate             16     594\n",
      "11       HS-grad              9   15784\n",
      "12       Masters             14    2657\n",
      "13     Preschool              1      83\n",
      "14   Prof-school             15     834\n",
      "15  Some-college             10   10878\n",
      "\n",
      "\n",
      "Encoding of marital-status column mapping: \n",
      "\n",
      "          marital-status  marital-status-num  counts\n",
      "0               Divorced                   2    6633\n",
      "1      Married-AF-spouse                   5      37\n",
      "2     Married-civ-spouse                   1   22379\n",
      "3  Married-spouse-absent                   3     628\n",
      "4          Never-married                   0   16117\n",
      "5              Separated                   4    1530\n",
      "6                Widowed                   6    1518\n",
      "\n",
      "\n",
      "Encoding of workclass column mapping: \n",
      "\n",
      "          workclass  workclass-num  counts\n",
      "0                 ?              0    2799\n",
      "1       Federal-gov              6    1432\n",
      "2         Local-gov              3    3136\n",
      "3      Never-worked              8      10\n",
      "4           Private              1   33906\n",
      "5      Self-emp-inc              5    1695\n",
      "6  Self-emp-not-inc              2    3862\n",
      "7         State-gov              4    1981\n",
      "8       Without-pay              7      21\n",
      "\n",
      "\n",
      "Encoding of native-country column mapping: \n",
      "\n",
      "                native-country  native-country-num  counts\n",
      "0                            ?                   0     857\n",
      "1                     Cambodia                  32      28\n",
      "2                       Canada                   5     182\n",
      "3                        China                  13     122\n",
      "4                     Columbia                  20      85\n",
      "5                         Cuba                   9     138\n",
      "6           Dominican-Republic                  15     103\n",
      "7                      Ecuador                  29      45\n",
      "8                  El-Salvador                   7     155\n",
      "9                      England                  10     127\n",
      "10                      France                  28      38\n",
      "11                     Germany                   4     206\n",
      "12                      Greece                  27      49\n",
      "13                   Guatemala                  17      88\n",
      "14                       Haiti                  22      75\n",
      "15          Holand-Netherlands                  41       1\n",
      "16                    Honduras                  39      20\n",
      "17                        Hong                  31      30\n",
      "18                     Hungary                  38      19\n",
      "19                       India                   8     151\n",
      "20                        Iran                  23      59\n",
      "21                     Ireland                  30      37\n",
      "22                       Italy                  14     105\n",
      "23                     Jamaica                  11     106\n",
      "24                       Japan                  18      92\n",
      "25                        Laos                  35      23\n",
      "26                      Mexico                   2     951\n",
      "27                   Nicaragua                  25      49\n",
      "28  Outlying-US(Guam-USVI-etc)                  37      23\n",
      "29                        Peru                  26      46\n",
      "30                 Philippines                   3     295\n",
      "31                      Poland                  19      87\n",
      "32                    Portugal                  24      67\n",
      "33                 Puerto-Rico                   6     184\n",
      "34                    Scotland                  40      21\n",
      "35                       South                  12     115\n",
      "36                      Taiwan                  21      65\n",
      "37                    Thailand                  34      30\n",
      "38             Trinadad&Tobago                  33      27\n",
      "39               United-States                   1   43832\n",
      "40                     Vietnam                  16      86\n",
      "41                  Yugoslavia                  36      23\n",
      "\n",
      "\n",
      "Encoding of sex column mapping: \n",
      "\n",
      "      sex  sex-num  counts\n",
      "0  Female        1   16192\n",
      "1    Male        0   32650\n",
      "\n",
      "\n",
      "Encoding of race column mapping: \n",
      "\n",
      "                 race  race-num  counts\n",
      "0  Amer-Indian-Eskimo         3     470\n",
      "1  Asian-Pac-Islander         2    1519\n",
      "2               Black         0    4685\n",
      "3               Other         4     406\n",
      "4               White         1   41762\n",
      "\n",
      "\n",
      "Encoding of relationship column mapping: \n",
      "\n",
      "     relationship  relationship-num  counts\n",
      "0         Husband                 1   19716\n",
      "1   Not-in-family                 0   12583\n",
      "2  Other-relative                 5    1506\n",
      "3       Own-child                 2    7581\n",
      "4       Unmarried                 3    5125\n",
      "5            Wife                 4    2331\n",
      "\n",
      "\n",
      "Encoding of age column mapping: \n",
      "\n",
      "    age-binned  age  counts\n",
      "0            0   17       0\n",
      "1            0   18       0\n",
      "2            0   19       0\n",
      "3            0   20       0\n",
      "4            0   21       0\n",
      "..         ...  ...     ...\n",
      "735          9   86       0\n",
      "736          9   87       0\n",
      "737          9   88       0\n",
      "738          9   89       0\n",
      "739          9   90       0\n",
      "\n",
      "[740 rows x 3 columns]\n",
      "\n",
      "\n",
      "Encoding of hours-per-week column mapping: \n",
      "\n",
      "    hours-per-week-binned  hours-per-week  counts\n",
      "0                       0               1      27\n",
      "1                       0               2      53\n",
      "2                       0               3      59\n",
      "3                       0               4      84\n",
      "4                       0               5      95\n",
      "..                    ...             ...     ...\n",
      "955                     9              95       2\n",
      "956                     9              96       9\n",
      "957                     9              97       2\n",
      "958                     9              98      14\n",
      "959                     9              99     137\n",
      "\n",
      "[960 rows x 3 columns]\n",
      "\n",
      "\n",
      "Encoding of capital-gain column mapping: \n",
      "\n",
      "     capital-gain-binned  capital-gain  counts\n",
      "0                      0             0   44807\n",
      "1                      0           114       8\n",
      "2                      0           401       5\n",
      "3                      0           594      52\n",
      "4                      0           914      10\n",
      "...                  ...           ...     ...\n",
      "1225                   9         25236       0\n",
      "1226                   9         27828       0\n",
      "1227                   9         34095       0\n",
      "1228                   9         41310       0\n",
      "1229                   9         99999     244\n",
      "\n",
      "[1230 rows x 3 columns]\n",
      "\n",
      "\n",
      "Encoding of capital-loss column mapping: \n",
      "\n",
      "    capital-loss-binned  capital-loss  counts\n",
      "0                     0             0   46560\n",
      "1                     0           155       1\n",
      "2                     0           213       5\n",
      "3                     0           323       5\n",
      "4                     0           419       3\n",
      "..                  ...           ...     ...\n",
      "985                   9          3175       0\n",
      "986                   9          3683       0\n",
      "987                   9          3770       0\n",
      "988                   9          3900       0\n",
      "989                   9          4356       0\n",
      "\n",
      "[990 rows x 3 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through encoded columns and print mapping and count of each column\n",
    "for i in encoded_columns_list:\n",
    "    print(\"Encoding of\", i, \"column mapping: \\n\")\n",
    "    print(ds_combined.groupby([i, i + '-num']).size().reset_index(name='counts'))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Iterate through encoded binned columns and print mapping and count of each column\n",
    "for i in encoded_columns_binned_list:\n",
    "    print(\"Encoding of\", i, \"column mapping: \\n\")\n",
    "    print(ds_combined.groupby([i + '-binned', i]).size().reset_index(name='counts'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "# Function to plot two bar charts side by side stacked crosstab\n",
    "def plot_crosstab(ds_col1, ds_col2, title, xlabel, ylabel, xticks_rotation=0, colors=['red', 'blue'], figsize=(10, 5)):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    # First subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plotgraph1 = pd.crosstab(index=ds_col1, columns=ds_col2)\n",
    "    plotgraph1.plot(ax=ax1, kind='bar', stacked=True, color=colors, grid=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=xticks_rotation)\n",
    "\n",
    "    # Second subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plotgraph2= pd.crosstab(index=ds_col1, columns=ds_col2, normalize='index')\n",
    "    plotgraph2.plot(ax=ax2, kind='bar', stacked=True, color=colors, grid=False)\n",
    "    plt.title(title + ' (normalized)')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=xticks_rotation)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plot pie chart with income-num column with palette='husl'\n",
    "    ds_combined['income-num'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['red', 'blue'], figsize=(5, 5), title='Income Distribution')\n",
    "\n",
    "    # Plot graph with occupation-num and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['occupation-num'], ds_combined['income-num'], 'Income based on Occupation', 'Occupation', 'Count', 90)\n",
    "\n",
    "    # Plot graph with education-num and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['education-num'], ds_combined['income-num'], 'Income based on Education', 'Education', 'Count', 90)\n",
    "\n",
    "    # Plot graph with workclass-num and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['workclass-num'], ds_combined['income-num'], 'Income based on Workclass', 'Workclass', 'Count', 90)\n",
    "\n",
    "    # Plot graph with marital-status-num and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['marital-status-num'], ds_combined['income-num'], 'Income based on Marital Status', 'Marital Status', 'Count', 90)\n",
    "\n",
    "    # Plot graph with native-country-num and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['native-country-num'], ds_combined['income-num'], 'Income based on Native Country', 'Native Country', 'Count', 90)\n",
    "\n",
    "    # Plot graph with race-num and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['race-num'],ds_combined['income-num'], 'Income based on Race', 'Race', 'Count', 90)\n",
    "\n",
    "    # Plot graph with sex-num and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['sex-num'], ds_combined['income-num'], 'Income based on Gender', 'Gender', 'Count', 90)\n",
    "\n",
    "    # Plot graph with age-binned and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['age-binned'], ds_combined['income-num'], 'Income based on Age', 'Age', 'Count', 90)\n",
    "\n",
    "    # Plot graph with hours-per-week-binned and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['hours-per-week-binned'], ds_combined['income-num'], 'Income based on Working Hours per Week', 'Working Hours per Week', 'Count', 90)\n",
    "\n",
    "    # Plot graph with capital-gain-binned and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['capital-gain-binned'], ds_combined['income-num'], 'Income based on Capital Gain', 'Capital Gain', 'Count', 90)\n",
    "\n",
    "    # Plot graph with capital-loss-binned and income-num columns crosstab stacked\n",
    "    plot_crosstab(ds_combined['capital-loss-binned'], ds_combined['income-num'], 'Income based on Capital Loss', 'Capital Loss', 'Count', 90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Data\n",
    "From the histogram and statistical summary above, there are notable data imbalance. The imbalance in representation for features such as education, gender, occupation, marital status, working hours, work class, and age may be a result of natural phenomena or demographics representation. However, the feature imbalance for **_income, native country, race_** are likely due to constraints from collection of data and can be of significance creating biased on the resulting model due to under representation. Deeper understanding of this implication should be further investigated in the future. Only a quick evaluation is conducted to look at the correlation of these variables with the primariy focus on the classifier algorithm function, and is a currrent limitation yet a common issue in real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "imbalanced_columns = ['income', 'native-country', 'race']\n",
    "imbalanced_columns_encoded = ['income-num', 'native-country-num', 'race-num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot scatter with encoded dataset using pairplot hue difference by income-num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plot pairplot of ds_combined with hue=income-num\n",
    "    sns.pairplot(ds_combined[ds_header_names_encoded], hue='income-num', plot_kws={'alpha': 0.1}, palette='husl', markers=['x', '+'])\n",
    "\n",
    "    # Save pairplot of ds_combined with hue=income-num to file\n",
    "    plt.savefig('ds_combined_encoded_pairplot_income-num.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot scatter with encoded dataset using pairploat hue difference by dataset source (train/ test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plot pairplot of ds_combined with hue=dataset\n",
    "    sns.pairplot(ds_combined[ds_header_names_encoded+['dataset']], hue='dataset', plot_kws={'alpha': 0.1}, palette='husl', markers=['x', '+'])\n",
    "\n",
    "    # Save pairplot of ds_combined with hue=dataset to file\n",
    "    plt.savefig('ds_combined_encoded_pairplot_dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection by correlation to income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature correlation sorted:\n",
      "education-num            0.332613\n",
      "hours-per-week-binned    0.226595\n",
      "age-binned               0.224554\n",
      "sex-num                  0.214628\n",
      "capital-gain-binned      0.177530\n",
      "workclass-num            0.154423\n",
      "capital-loss-binned      0.121946\n",
      "occupation-num           0.076722\n",
      "relationship-num         0.054008\n",
      "race-num                 0.033723\n",
      "native-country-num       0.016793\n",
      "fnlwgt                   0.006339\n",
      "marital-status-num       0.002517\n",
      "Name: income-num, dtype: float64 \n",
      "\n",
      "Significant features:\n",
      "['age-binned', 'workclass-num', 'education-num', 'occupation-num', 'relationship-num', 'race-num', 'sex-num', 'capital-gain-binned', 'capital-loss-binned', 'hours-per-week-binned', 'native-country-num', 'income-num'] \n",
      "\n",
      "Insignificant features:\n",
      "['fnlwgt', 'marital-status-num'] \n",
      "\n",
      "Columns in ds_combined not required (excluding ['age', 'hours-per-week', 'capital-gain', 'capital-loss']):\n",
      "{'relationship', 'income', 'sex', 'workclass', 'native-country', 'education', 'dataset', 'race', 'marital-status', 'occupation'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create list of feature correlation (abs) with income-num\n",
    "feature_correlation = ds_combined[ds_header_names_encoded].corr()['income-num']\n",
    "\n",
    "# Print feature_correlation sorted\n",
    "print(\"Feature correlation sorted:\")\n",
    "print(feature_correlation.drop('income-num').abs().sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "# Identify significant features with correlation by setting threshold\n",
    "significant_features = feature_correlation[feature_correlation.abs() > 0.01].index.tolist()\n",
    "print(\"Significant features:\")\n",
    "print(significant_features, \"\\n\")\n",
    "\n",
    "# Identify insignificant features with correlation by setting threshold\n",
    "insignificant_features = feature_correlation[feature_correlation.abs() <= 0.01].index.tolist()\n",
    "print(\"Insignificant features:\")\n",
    "print(insignificant_features, \"\\n\")\n",
    "\n",
    "# Print columns in ds_combined not in ds_header_names_encoded + age + hours-per-week + capital-gain + capital-loss\n",
    "significant_features_original = ['age', 'hours-per-week', 'capital-gain', 'capital-loss']\n",
    "print(f\"Columns in ds_combined not required (excluding {significant_features_original}):\")\n",
    "required_columns = set(ds_header_names_encoded + significant_features_original)\n",
    "unrequired_columns = set(ds_combined.columns) - required_columns\n",
    "print(unrequired_columns, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plot heatmap of ds_combined encoded with correlation\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(ds_combined[ds_header_names_encoded].corr(), annot=True, cmap='coolwarm', linewidths=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important features will be retained while dropping features with low correlation. Columns not required can also be dropped.\n",
    ">Note: From comparison, binning seems to lower the correlation value to target, i.e. capital-gained-binned has lower correlation than capital-gained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarized count of missing values in original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age : 0\n",
      "workclass : 2799\n",
      "fnlwgt : 0\n",
      "education : 0\n",
      "education-num : 0\n",
      "marital-status : 0\n",
      "occupation : 2809\n",
      "relationship : 0\n",
      "race : 0\n",
      "sex : 0\n",
      "capital-gain : 0\n",
      "capital-loss : 0\n",
      "hours-per-week : 0\n",
      "native-country : 857\n",
      "income : 0\n"
     ]
    }
   ],
   "source": [
    "# Count ? values in each column\n",
    "for i in ds_combined[ds_header_names]:\n",
    "    print(i, \":\", ds_combined[ds_combined[i] == '?'][i].count())\n",
    "\n",
    "missing_columns = ['workclass', 'occupation', 'native-country']\n",
    "missing_columns_encoded = ['workclass-num', 'occupation-num', 'native-country-num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the count and percentage of missing values relative to the groups for a column\n",
    "Focusing on the top correlated features to income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "# Function to count number of ? in each column, and percentage of ? in each column group\n",
    "def count_missing_values(dataset, dataset_name = \"dataset\"):\n",
    "    print(f\"Missing values in {dataset_name}:\")\n",
    "    for col in dataset.columns:\n",
    "        print(col, \": \", dataset[col].isin(['?']).sum(), \" \", dataset[col].isin(['?']).sum()/dataset.shape[0])\n",
    "\n",
    "# Function to generate table with count of missing values in workclass, occupation, native-country columns respectively. Add columns with percentage of missing values in each column, split by dataset column = train or test\n",
    "def generate_table_missing_count_for_won_by_column(ds, group_by_column_name=['dataset']):\n",
    "    return ds.groupby(group_by_column_name).apply(lambda x: pd.Series({\n",
    "        'workclass_missing_values_count': x['workclass'].isin(['?']).sum(),\n",
    "        'occupation_missing_values_count': x['occupation'].isin(['?']).sum(),\n",
    "        'native-country_missing_values_count': x['native-country'].isin(['?']).sum(),\n",
    "        'workclass_missing_values_percentage': x['workclass'].isin(['?']).sum()/x.shape[0],\n",
    "        'occupation_missing_values_percentage': x['occupation'].isin(['?']).sum()/x.shape[0],\n",
    "        'native-country_missing_values_percentage': x['native-country'].isin(['?']).sum()/x.shape[0]\n",
    "    }) if x.shape[0] > 0 else pd.Series({\n",
    "        'workclass_missing_values_count': 0,\n",
    "        'occupation_missing_values_count': 0,\n",
    "        'native-country_missing_values_count': 0,\n",
    "        'workclass_missing_values_percentage': 0,\n",
    "        'occupation_missing_values_percentage': 0,\n",
    "        'native-country_missing_values_percentage': 0\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Generate a table with count of missing values in workclass, occupation, native-country columns respectively. Add columns with percentage of missing values in each column, split by dataset column = train or test\n",
    "    ds_combined_missing_values = generate_table_missing_count_for_won_by_column(ds_combined)\n",
    "\n",
    "    # Plot two bar charts of ds_combined_missing_values. One by count of missing values, one by percentage of missing values. Using subplot to plot side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # First subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    ds_combined_missing_values[['workclass_missing_values_count', 'occupation_missing_values_count', 'native-country_missing_values_count']].plot(kind='bar', ax=ax1, title='Missing Values in Combined Dataset', rot=0)\n",
    "\n",
    "    # Second subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    ds_combined_missing_values[['workclass_missing_values_percentage', 'occupation_missing_values_percentage', 'native-country_missing_values_percentage']].plot(kind='bar', ax=ax2, title='Missing Values Percentage in Combined Dataset', rot=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Iterate through top significant features and plot two bar charts of ds_combined_missing_values.\n",
    "    for i in feature_correlation.drop('income-num').abs().sort_values(ascending=False).head(5).index.tolist():\n",
    "        print(\"Plotting missing values for\", i, \"column\")\n",
    "        # Generate table with count of missing values in workclass, occupation, native-country columns respectively. Add columns with percentage of missing values in each column, split by i column\n",
    "        ds_combined_missing_values = generate_table_missing_count_for_won_by_column(ds_combined, group_by_column_name=[i])\n",
    "        # Plot two bar charts of ds_combined_missing_values. One by count of missing values, one by percentage of missing values. Using subplot to plot side by side\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # First subplot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        ds_combined_missing_values[['workclass_missing_values_count', 'occupation_missing_values_count', 'native-country_missing_values_count']].plot(kind='bar', ax=ax1, title='Missing Values in Combined Dataset', rot=90)\n",
    "\n",
    "        # Second subplot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        ds_combined_missing_values[['workclass_missing_values_percentage', 'occupation_missing_values_percentage', 'native-country_missing_values_percentage']].plot(kind='bar', ax=ax2, title='Missing Values Percentage in Combined Dataset', rot=90)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the missing value count and percentage in workclass, occupation, native-country columns individually for each group of highly imbalanced variables (income, race, native-country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "# Function to generate count of ? in specified column group by another column\n",
    "def generate_missing_values_count_by_column(dataset, column_name, group_by_column_name, missing_value_indicator = 0):\n",
    "    # Generate total count of records by group\n",
    "    dataset_missing_values = dataset.groupby(group_by_column_name).size().reset_index(name='total_count')\n",
    "    # Generate count of missing values in column_name by group\n",
    "    # Initialize missing_values_count column with 0\n",
    "    dataset_missing_values['missing_values_count'] = 0\n",
    "    # Initialize mssing_value_percentage column with 0\n",
    "    dataset_missing_values['missing_values_percentage'] = 0\n",
    "    # Iterate through each group, check if total_count is 0\n",
    "    for index, row in dataset_missing_values.iterrows():\n",
    "        if row['total_count'] > 0:\n",
    "            dataset_missing_values.loc[index, 'missing_values_count'] = dataset[(dataset[column_name] == missing_value_indicator) & (dataset[group_by_column_name] == row[group_by_column_name])].shape[0]\n",
    "            dataset_missing_values.loc[index, 'missing_values_percentage'] = dataset_missing_values.loc[index, 'missing_values_count']/dataset_missing_values.loc[index, 'total_count'] * 100\n",
    "    return dataset_missing_values.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "Visualization"
    ]
   },
   "outputs": [],
   "source": [
    "if not G_SKIP_VISUALIZATION:\n",
    "    # Plot two graphs using subplot side by side, showing the  missing value count/ percentage on workclass/ occupation/ native-country column, group by income/ race/ native-country\n",
    "    # Iterate through income, race, native-country\n",
    "    for i in imbalanced_columns_encoded:\n",
    "        # Iterate through workclass, occupation, native-country\n",
    "        for j in missing_columns_encoded:\n",
    "            if i != j:\n",
    "                print(\"Plotting missing values for\", j, \"column, grouped by\", i, \"column\")\n",
    "                # Generate count of ? in j column, group by i\n",
    "                ds_combined_missing_values = generate_missing_values_count_by_column(ds_combined, j, i)\n",
    "                # Plot two bar charts of ds_combined_missing_values. One by count of missing values, one by percentage of missing values. Using subplot to plot side by side\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "                # First subplot\n",
    "                plt.subplot(1, 2, 1)\n",
    "                ds_combined_missing_values[['missing_values_count']].plot(kind='bar', ax=ax1, title=f'Missing Values in {j}\\ngroup by {i}', rot=90)\n",
    "\n",
    "                # Second subplot\n",
    "                plt.subplot(1, 2, 2)\n",
    "                ds_combined_missing_values[['missing_values_percentage']].plot(kind='bar', ax=ax2, title=f'Missing Values Percentage in {j}\\ngroup by {i}', rot=90)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column to label records with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows with missing values:  7.411653904426519 %\n",
      "Percentage of rows with missing values in native-country column to total rows with missing values:  22.40331491712707 %\n",
      "Percentage of rows with missing values in native-country column to total rows:  1.660456164776217 %\n",
      "\n",
      "\n",
      "Count and percentage of missing values grouped by income: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1bcee\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1bcee_level0_col0\" class=\"col_heading level0 col0\" >income</th>\n",
       "      <th id=\"T_1bcee_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_1bcee_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_1bcee_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1bcee_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1bcee_row0_col0\" class=\"data row0 col0\" ><=50K</td>\n",
       "      <td id=\"T_1bcee_row0_col1\" class=\"data row0 col1\" >37155</td>\n",
       "      <td id=\"T_1bcee_row0_col2\" class=\"data row0 col2\" >3141</td>\n",
       "      <td id=\"T_1bcee_row0_col3\" class=\"data row0 col3\" >8.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1bcee_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1bcee_row1_col0\" class=\"data row1 col0\" >>50K</td>\n",
       "      <td id=\"T_1bcee_row1_col1\" class=\"data row1 col1\" >11687</td>\n",
       "      <td id=\"T_1bcee_row1_col2\" class=\"data row1 col2\" >479</td>\n",
       "      <td id=\"T_1bcee_row1_col3\" class=\"data row1 col3\" >4.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb466461c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and percentage of missing values grouped by native-country: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_60973\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_60973_level0_col0\" class=\"col_heading level0 col0\" >native-country</th>\n",
       "      <th id=\"T_60973_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_60973_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_60973_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_60973_row0_col0\" class=\"data row0 col0\" >?</td>\n",
       "      <td id=\"T_60973_row0_col1\" class=\"data row0 col1\" >857</td>\n",
       "      <td id=\"T_60973_row0_col2\" class=\"data row0 col2\" >857</td>\n",
       "      <td id=\"T_60973_row0_col3\" class=\"data row0 col3\" >100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_60973_row1_col0\" class=\"data row1 col0\" >Cambodia</td>\n",
       "      <td id=\"T_60973_row1_col1\" class=\"data row1 col1\" >28</td>\n",
       "      <td id=\"T_60973_row1_col2\" class=\"data row1 col2\" >2</td>\n",
       "      <td id=\"T_60973_row1_col3\" class=\"data row1 col3\" >7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_60973_row2_col0\" class=\"data row2 col0\" >Canada</td>\n",
       "      <td id=\"T_60973_row2_col1\" class=\"data row2 col1\" >182</td>\n",
       "      <td id=\"T_60973_row2_col2\" class=\"data row2 col2\" >19</td>\n",
       "      <td id=\"T_60973_row2_col3\" class=\"data row2 col3\" >10.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_60973_row3_col0\" class=\"data row3 col0\" >China</td>\n",
       "      <td id=\"T_60973_row3_col1\" class=\"data row3 col1\" >122</td>\n",
       "      <td id=\"T_60973_row3_col2\" class=\"data row3 col2\" >9</td>\n",
       "      <td id=\"T_60973_row3_col3\" class=\"data row3 col3\" >7.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_60973_row4_col0\" class=\"data row4 col0\" >Columbia</td>\n",
       "      <td id=\"T_60973_row4_col1\" class=\"data row4 col1\" >85</td>\n",
       "      <td id=\"T_60973_row4_col2\" class=\"data row4 col2\" >3</td>\n",
       "      <td id=\"T_60973_row4_col3\" class=\"data row4 col3\" >3.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_60973_row5_col0\" class=\"data row5 col0\" >Cuba</td>\n",
       "      <td id=\"T_60973_row5_col1\" class=\"data row5 col1\" >138</td>\n",
       "      <td id=\"T_60973_row5_col2\" class=\"data row5 col2\" >5</td>\n",
       "      <td id=\"T_60973_row5_col3\" class=\"data row5 col3\" >3.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_60973_row6_col0\" class=\"data row6 col0\" >Dominican-Republic</td>\n",
       "      <td id=\"T_60973_row6_col1\" class=\"data row6 col1\" >103</td>\n",
       "      <td id=\"T_60973_row6_col2\" class=\"data row6 col2\" >6</td>\n",
       "      <td id=\"T_60973_row6_col3\" class=\"data row6 col3\" >5.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_60973_row7_col0\" class=\"data row7 col0\" >Ecuador</td>\n",
       "      <td id=\"T_60973_row7_col1\" class=\"data row7 col1\" >45</td>\n",
       "      <td id=\"T_60973_row7_col2\" class=\"data row7 col2\" >2</td>\n",
       "      <td id=\"T_60973_row7_col3\" class=\"data row7 col3\" >4.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_60973_row8_col0\" class=\"data row8 col0\" >El-Salvador</td>\n",
       "      <td id=\"T_60973_row8_col1\" class=\"data row8 col1\" >155</td>\n",
       "      <td id=\"T_60973_row8_col2\" class=\"data row8 col2\" >8</td>\n",
       "      <td id=\"T_60973_row8_col3\" class=\"data row8 col3\" >5.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_60973_row9_col0\" class=\"data row9 col0\" >England</td>\n",
       "      <td id=\"T_60973_row9_col1\" class=\"data row9 col1\" >127</td>\n",
       "      <td id=\"T_60973_row9_col2\" class=\"data row9 col2\" >8</td>\n",
       "      <td id=\"T_60973_row9_col3\" class=\"data row9 col3\" >6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_60973_row10_col0\" class=\"data row10 col0\" >France</td>\n",
       "      <td id=\"T_60973_row10_col1\" class=\"data row10 col1\" >38</td>\n",
       "      <td id=\"T_60973_row10_col2\" class=\"data row10 col2\" >2</td>\n",
       "      <td id=\"T_60973_row10_col3\" class=\"data row10 col3\" >5.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_60973_row11_col0\" class=\"data row11 col0\" >Germany</td>\n",
       "      <td id=\"T_60973_row11_col1\" class=\"data row11 col1\" >206</td>\n",
       "      <td id=\"T_60973_row11_col2\" class=\"data row11 col2\" >13</td>\n",
       "      <td id=\"T_60973_row11_col3\" class=\"data row11 col3\" >6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_60973_row12_col0\" class=\"data row12 col0\" >Greece</td>\n",
       "      <td id=\"T_60973_row12_col1\" class=\"data row12 col1\" >49</td>\n",
       "      <td id=\"T_60973_row12_col2\" class=\"data row12 col2\" >0</td>\n",
       "      <td id=\"T_60973_row12_col3\" class=\"data row12 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_60973_row13_col0\" class=\"data row13 col0\" >Guatemala</td>\n",
       "      <td id=\"T_60973_row13_col1\" class=\"data row13 col1\" >88</td>\n",
       "      <td id=\"T_60973_row13_col2\" class=\"data row13 col2\" >2</td>\n",
       "      <td id=\"T_60973_row13_col3\" class=\"data row13 col3\" >2.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_60973_row14_col0\" class=\"data row14 col0\" >Haiti</td>\n",
       "      <td id=\"T_60973_row14_col1\" class=\"data row14 col1\" >75</td>\n",
       "      <td id=\"T_60973_row14_col2\" class=\"data row14 col2\" >6</td>\n",
       "      <td id=\"T_60973_row14_col3\" class=\"data row14 col3\" >8.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_60973_row15_col0\" class=\"data row15 col0\" >Holand-Netherlands</td>\n",
       "      <td id=\"T_60973_row15_col1\" class=\"data row15 col1\" >1</td>\n",
       "      <td id=\"T_60973_row15_col2\" class=\"data row15 col2\" >0</td>\n",
       "      <td id=\"T_60973_row15_col3\" class=\"data row15 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_60973_row16_col0\" class=\"data row16 col0\" >Honduras</td>\n",
       "      <td id=\"T_60973_row16_col1\" class=\"data row16 col1\" >20</td>\n",
       "      <td id=\"T_60973_row16_col2\" class=\"data row16 col2\" >1</td>\n",
       "      <td id=\"T_60973_row16_col3\" class=\"data row16 col3\" >5.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_60973_row17_col0\" class=\"data row17 col0\" >Hong</td>\n",
       "      <td id=\"T_60973_row17_col1\" class=\"data row17 col1\" >30</td>\n",
       "      <td id=\"T_60973_row17_col2\" class=\"data row17 col2\" >2</td>\n",
       "      <td id=\"T_60973_row17_col3\" class=\"data row17 col3\" >6.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_60973_row18_col0\" class=\"data row18 col0\" >Hungary</td>\n",
       "      <td id=\"T_60973_row18_col1\" class=\"data row18 col1\" >19</td>\n",
       "      <td id=\"T_60973_row18_col2\" class=\"data row18 col2\" >1</td>\n",
       "      <td id=\"T_60973_row18_col3\" class=\"data row18 col3\" >5.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_60973_row19_col0\" class=\"data row19 col0\" >India</td>\n",
       "      <td id=\"T_60973_row19_col1\" class=\"data row19 col1\" >151</td>\n",
       "      <td id=\"T_60973_row19_col2\" class=\"data row19 col2\" >4</td>\n",
       "      <td id=\"T_60973_row19_col3\" class=\"data row19 col3\" >2.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_60973_row20_col0\" class=\"data row20 col0\" >Iran</td>\n",
       "      <td id=\"T_60973_row20_col1\" class=\"data row20 col1\" >59</td>\n",
       "      <td id=\"T_60973_row20_col2\" class=\"data row20 col2\" >3</td>\n",
       "      <td id=\"T_60973_row20_col3\" class=\"data row20 col3\" >5.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_60973_row21_col0\" class=\"data row21 col0\" >Ireland</td>\n",
       "      <td id=\"T_60973_row21_col1\" class=\"data row21 col1\" >37</td>\n",
       "      <td id=\"T_60973_row21_col2\" class=\"data row21 col2\" >1</td>\n",
       "      <td id=\"T_60973_row21_col3\" class=\"data row21 col3\" >2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_60973_row22_col0\" class=\"data row22 col0\" >Italy</td>\n",
       "      <td id=\"T_60973_row22_col1\" class=\"data row22 col1\" >105</td>\n",
       "      <td id=\"T_60973_row22_col2\" class=\"data row22 col2\" >5</td>\n",
       "      <td id=\"T_60973_row22_col3\" class=\"data row22 col3\" >4.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_60973_row23_col0\" class=\"data row23 col0\" >Jamaica</td>\n",
       "      <td id=\"T_60973_row23_col1\" class=\"data row23 col1\" >106</td>\n",
       "      <td id=\"T_60973_row23_col2\" class=\"data row23 col2\" >3</td>\n",
       "      <td id=\"T_60973_row23_col3\" class=\"data row23 col3\" >2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_60973_row24_col0\" class=\"data row24 col0\" >Japan</td>\n",
       "      <td id=\"T_60973_row24_col1\" class=\"data row24 col1\" >92</td>\n",
       "      <td id=\"T_60973_row24_col2\" class=\"data row24 col2\" >3</td>\n",
       "      <td id=\"T_60973_row24_col3\" class=\"data row24 col3\" >3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_60973_row25_col0\" class=\"data row25 col0\" >Laos</td>\n",
       "      <td id=\"T_60973_row25_col1\" class=\"data row25 col1\" >23</td>\n",
       "      <td id=\"T_60973_row25_col2\" class=\"data row25 col2\" >2</td>\n",
       "      <td id=\"T_60973_row25_col3\" class=\"data row25 col3\" >8.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_60973_row26_col0\" class=\"data row26 col0\" >Mexico</td>\n",
       "      <td id=\"T_60973_row26_col1\" class=\"data row26 col1\" >951</td>\n",
       "      <td id=\"T_60973_row26_col2\" class=\"data row26 col2\" >48</td>\n",
       "      <td id=\"T_60973_row26_col3\" class=\"data row26 col3\" >5.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_60973_row27_col0\" class=\"data row27 col0\" >Nicaragua</td>\n",
       "      <td id=\"T_60973_row27_col1\" class=\"data row27 col1\" >49</td>\n",
       "      <td id=\"T_60973_row27_col2\" class=\"data row27 col2\" >1</td>\n",
       "      <td id=\"T_60973_row27_col3\" class=\"data row27 col3\" >2.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_60973_row28_col0\" class=\"data row28 col0\" >Outlying-US(Guam-USVI-etc)</td>\n",
       "      <td id=\"T_60973_row28_col1\" class=\"data row28 col1\" >23</td>\n",
       "      <td id=\"T_60973_row28_col2\" class=\"data row28 col2\" >1</td>\n",
       "      <td id=\"T_60973_row28_col3\" class=\"data row28 col3\" >4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_60973_row29_col0\" class=\"data row29 col0\" >Peru</td>\n",
       "      <td id=\"T_60973_row29_col1\" class=\"data row29 col1\" >46</td>\n",
       "      <td id=\"T_60973_row29_col2\" class=\"data row29 col2\" >1</td>\n",
       "      <td id=\"T_60973_row29_col3\" class=\"data row29 col3\" >2.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_60973_row30_col0\" class=\"data row30 col0\" >Philippines</td>\n",
       "      <td id=\"T_60973_row30_col1\" class=\"data row30 col1\" >295</td>\n",
       "      <td id=\"T_60973_row30_col2\" class=\"data row30 col2\" >12</td>\n",
       "      <td id=\"T_60973_row30_col3\" class=\"data row30 col3\" >4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_60973_row31_col0\" class=\"data row31 col0\" >Poland</td>\n",
       "      <td id=\"T_60973_row31_col1\" class=\"data row31 col1\" >87</td>\n",
       "      <td id=\"T_60973_row31_col2\" class=\"data row31 col2\" >6</td>\n",
       "      <td id=\"T_60973_row31_col3\" class=\"data row31 col3\" >6.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_60973_row32_col0\" class=\"data row32 col0\" >Portugal</td>\n",
       "      <td id=\"T_60973_row32_col1\" class=\"data row32 col1\" >67</td>\n",
       "      <td id=\"T_60973_row32_col2\" class=\"data row32 col2\" >5</td>\n",
       "      <td id=\"T_60973_row32_col3\" class=\"data row32 col3\" >7.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_60973_row33_col0\" class=\"data row33 col0\" >Puerto-Rico</td>\n",
       "      <td id=\"T_60973_row33_col1\" class=\"data row33 col1\" >184</td>\n",
       "      <td id=\"T_60973_row33_col2\" class=\"data row33 col2\" >9</td>\n",
       "      <td id=\"T_60973_row33_col3\" class=\"data row33 col3\" >4.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_60973_row34_col0\" class=\"data row34 col0\" >Scotland</td>\n",
       "      <td id=\"T_60973_row34_col1\" class=\"data row34 col1\" >21</td>\n",
       "      <td id=\"T_60973_row34_col2\" class=\"data row34 col2\" >1</td>\n",
       "      <td id=\"T_60973_row34_col3\" class=\"data row34 col3\" >4.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_60973_row35_col0\" class=\"data row35 col0\" >South</td>\n",
       "      <td id=\"T_60973_row35_col1\" class=\"data row35 col1\" >115</td>\n",
       "      <td id=\"T_60973_row35_col2\" class=\"data row35 col2\" >14</td>\n",
       "      <td id=\"T_60973_row35_col3\" class=\"data row35 col3\" >12.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_60973_row36_col0\" class=\"data row36 col0\" >Taiwan</td>\n",
       "      <td id=\"T_60973_row36_col1\" class=\"data row36 col1\" >65</td>\n",
       "      <td id=\"T_60973_row36_col2\" class=\"data row36 col2\" >10</td>\n",
       "      <td id=\"T_60973_row36_col3\" class=\"data row36 col3\" >15.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_60973_row37_col0\" class=\"data row37 col0\" >Thailand</td>\n",
       "      <td id=\"T_60973_row37_col1\" class=\"data row37 col1\" >30</td>\n",
       "      <td id=\"T_60973_row37_col2\" class=\"data row37 col2\" >1</td>\n",
       "      <td id=\"T_60973_row37_col3\" class=\"data row37 col3\" >3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_60973_row38_col0\" class=\"data row38 col0\" >Trinadad&Tobago</td>\n",
       "      <td id=\"T_60973_row38_col1\" class=\"data row38 col1\" >27</td>\n",
       "      <td id=\"T_60973_row38_col2\" class=\"data row38 col2\" >1</td>\n",
       "      <td id=\"T_60973_row38_col3\" class=\"data row38 col3\" >3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_60973_row39_col0\" class=\"data row39 col0\" >United-States</td>\n",
       "      <td id=\"T_60973_row39_col1\" class=\"data row39 col1\" >43832</td>\n",
       "      <td id=\"T_60973_row39_col2\" class=\"data row39 col2\" >2540</td>\n",
       "      <td id=\"T_60973_row39_col3\" class=\"data row39 col3\" >5.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_60973_row40_col0\" class=\"data row40 col0\" >Vietnam</td>\n",
       "      <td id=\"T_60973_row40_col1\" class=\"data row40 col1\" >86</td>\n",
       "      <td id=\"T_60973_row40_col2\" class=\"data row40 col2\" >3</td>\n",
       "      <td id=\"T_60973_row40_col3\" class=\"data row40 col3\" >3.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60973_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_60973_row41_col0\" class=\"data row41 col0\" >Yugoslavia</td>\n",
       "      <td id=\"T_60973_row41_col1\" class=\"data row41 col1\" >23</td>\n",
       "      <td id=\"T_60973_row41_col2\" class=\"data row41 col2\" >0</td>\n",
       "      <td id=\"T_60973_row41_col3\" class=\"data row41 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb40c3e5e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and percentage of missing values grouped by race: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_09817\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_09817_level0_col0\" class=\"col_heading level0 col0\" >race</th>\n",
       "      <th id=\"T_09817_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_09817_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_09817_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_09817_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_09817_row0_col0\" class=\"data row0 col0\" >Amer-Indian-Eskimo</td>\n",
       "      <td id=\"T_09817_row0_col1\" class=\"data row0 col1\" >470</td>\n",
       "      <td id=\"T_09817_row0_col2\" class=\"data row0 col2\" >35</td>\n",
       "      <td id=\"T_09817_row0_col3\" class=\"data row0 col3\" >7.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_09817_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_09817_row1_col0\" class=\"data row1 col0\" >Asian-Pac-Islander</td>\n",
       "      <td id=\"T_09817_row1_col1\" class=\"data row1 col1\" >1519</td>\n",
       "      <td id=\"T_09817_row1_col2\" class=\"data row1 col2\" >216</td>\n",
       "      <td id=\"T_09817_row1_col3\" class=\"data row1 col3\" >14.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_09817_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_09817_row2_col0\" class=\"data row2 col0\" >Black</td>\n",
       "      <td id=\"T_09817_row2_col1\" class=\"data row2 col1\" >4685</td>\n",
       "      <td id=\"T_09817_row2_col2\" class=\"data row2 col2\" >457</td>\n",
       "      <td id=\"T_09817_row2_col3\" class=\"data row2 col3\" >9.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_09817_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_09817_row3_col0\" class=\"data row3 col0\" >Other</td>\n",
       "      <td id=\"T_09817_row3_col1\" class=\"data row3 col1\" >406</td>\n",
       "      <td id=\"T_09817_row3_col2\" class=\"data row3 col2\" >53</td>\n",
       "      <td id=\"T_09817_row3_col3\" class=\"data row3 col3\" >13.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_09817_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_09817_row4_col0\" class=\"data row4 col0\" >White</td>\n",
       "      <td id=\"T_09817_row4_col1\" class=\"data row4 col1\" >41762</td>\n",
       "      <td id=\"T_09817_row4_col2\" class=\"data row4 col2\" >2859</td>\n",
       "      <td id=\"T_09817_row4_col3\" class=\"data row4 col3\" >6.8%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb45d1baf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and percentage of missing values grouped by education-num: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a7405\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a7405_level0_col0\" class=\"col_heading level0 col0\" >education-num</th>\n",
       "      <th id=\"T_a7405_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_a7405_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_a7405_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a7405_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_a7405_row0_col1\" class=\"data row0 col1\" >83</td>\n",
       "      <td id=\"T_a7405_row0_col2\" class=\"data row0 col2\" >11</td>\n",
       "      <td id=\"T_a7405_row0_col3\" class=\"data row0 col3\" >13.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a7405_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_a7405_row1_col1\" class=\"data row1 col1\" >247</td>\n",
       "      <td id=\"T_a7405_row1_col2\" class=\"data row1 col2\" >25</td>\n",
       "      <td id=\"T_a7405_row1_col3\" class=\"data row1 col3\" >10.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a7405_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_a7405_row2_col1\" class=\"data row2 col1\" >509</td>\n",
       "      <td id=\"T_a7405_row2_col2\" class=\"data row2 col2\" >60</td>\n",
       "      <td id=\"T_a7405_row2_col3\" class=\"data row2 col3\" >11.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a7405_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_a7405_row3_col1\" class=\"data row3 col1\" >955</td>\n",
       "      <td id=\"T_a7405_row3_col2\" class=\"data row3 col2\" >132</td>\n",
       "      <td id=\"T_a7405_row3_col3\" class=\"data row3 col3\" >13.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a7405_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_a7405_row4_col1\" class=\"data row4 col1\" >756</td>\n",
       "      <td id=\"T_a7405_row4_col2\" class=\"data row4 col2\" >80</td>\n",
       "      <td id=\"T_a7405_row4_col3\" class=\"data row4 col3\" >10.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a7405_row5_col0\" class=\"data row5 col0\" >6</td>\n",
       "      <td id=\"T_a7405_row5_col1\" class=\"data row5 col1\" >1389</td>\n",
       "      <td id=\"T_a7405_row5_col2\" class=\"data row5 col2\" >166</td>\n",
       "      <td id=\"T_a7405_row5_col3\" class=\"data row5 col3\" >12.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a7405_row6_col0\" class=\"data row6 col0\" >7</td>\n",
       "      <td id=\"T_a7405_row6_col1\" class=\"data row6 col1\" >1812</td>\n",
       "      <td id=\"T_a7405_row6_col2\" class=\"data row6 col2\" >193</td>\n",
       "      <td id=\"T_a7405_row6_col3\" class=\"data row6 col3\" >10.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a7405_row7_col0\" class=\"data row7 col0\" >8</td>\n",
       "      <td id=\"T_a7405_row7_col1\" class=\"data row7 col1\" >657</td>\n",
       "      <td id=\"T_a7405_row7_col2\" class=\"data row7 col2\" >80</td>\n",
       "      <td id=\"T_a7405_row7_col3\" class=\"data row7 col3\" >12.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a7405_row8_col0\" class=\"data row8 col0\" >9</td>\n",
       "      <td id=\"T_a7405_row8_col1\" class=\"data row8 col1\" >15784</td>\n",
       "      <td id=\"T_a7405_row8_col2\" class=\"data row8 col2\" >1001</td>\n",
       "      <td id=\"T_a7405_row8_col3\" class=\"data row8 col3\" >6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a7405_row9_col0\" class=\"data row9 col0\" >10</td>\n",
       "      <td id=\"T_a7405_row9_col1\" class=\"data row9 col1\" >10878</td>\n",
       "      <td id=\"T_a7405_row9_col2\" class=\"data row9 col2\" >979</td>\n",
       "      <td id=\"T_a7405_row9_col3\" class=\"data row9 col3\" >9.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_a7405_row10_col0\" class=\"data row10 col0\" >11</td>\n",
       "      <td id=\"T_a7405_row10_col1\" class=\"data row10 col1\" >2061</td>\n",
       "      <td id=\"T_a7405_row10_col2\" class=\"data row10 col2\" >102</td>\n",
       "      <td id=\"T_a7405_row10_col3\" class=\"data row10 col3\" >4.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_a7405_row11_col0\" class=\"data row11 col0\" >12</td>\n",
       "      <td id=\"T_a7405_row11_col1\" class=\"data row11 col1\" >1601</td>\n",
       "      <td id=\"T_a7405_row11_col2\" class=\"data row11 col2\" >94</td>\n",
       "      <td id=\"T_a7405_row11_col3\" class=\"data row11 col3\" >5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_a7405_row12_col0\" class=\"data row12 col0\" >13</td>\n",
       "      <td id=\"T_a7405_row12_col1\" class=\"data row12 col1\" >8025</td>\n",
       "      <td id=\"T_a7405_row12_col2\" class=\"data row12 col2\" >455</td>\n",
       "      <td id=\"T_a7405_row12_col3\" class=\"data row12 col3\" >5.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_a7405_row13_col0\" class=\"data row13 col0\" >14</td>\n",
       "      <td id=\"T_a7405_row13_col1\" class=\"data row13 col1\" >2657</td>\n",
       "      <td id=\"T_a7405_row13_col2\" class=\"data row13 col2\" >143</td>\n",
       "      <td id=\"T_a7405_row13_col3\" class=\"data row13 col3\" >5.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_a7405_row14_col0\" class=\"data row14 col0\" >15</td>\n",
       "      <td id=\"T_a7405_row14_col1\" class=\"data row14 col1\" >834</td>\n",
       "      <td id=\"T_a7405_row14_col2\" class=\"data row14 col2\" >49</td>\n",
       "      <td id=\"T_a7405_row14_col3\" class=\"data row14 col3\" >5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7405_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_a7405_row15_col0\" class=\"data row15 col0\" >16</td>\n",
       "      <td id=\"T_a7405_row15_col1\" class=\"data row15 col1\" >594</td>\n",
       "      <td id=\"T_a7405_row15_col2\" class=\"data row15 col2\" >50</td>\n",
       "      <td id=\"T_a7405_row15_col3\" class=\"data row15 col3\" >8.4%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb45d5f970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and percentage of missing values grouped by hours-per-week-binned: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1f5b3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1f5b3_level0_col0\" class=\"col_heading level0 col0\" >hours-per-week-binned</th>\n",
       "      <th id=\"T_1f5b3_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_1f5b3_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_1f5b3_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1f5b3_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_1f5b3_row0_col1\" class=\"data row0 col1\" >1125</td>\n",
       "      <td id=\"T_1f5b3_row0_col2\" class=\"data row0 col2\" >348</td>\n",
       "      <td id=\"T_1f5b3_row0_col3\" class=\"data row0 col3\" >30.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1f5b3_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_1f5b3_row1_col1\" class=\"data row1 col1\" >3328</td>\n",
       "      <td id=\"T_1f5b3_row1_col2\" class=\"data row1 col2\" >503</td>\n",
       "      <td id=\"T_1f5b3_row1_col3\" class=\"data row1 col3\" >15.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1f5b3_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_1f5b3_row2_col1\" class=\"data row2 col1\" >3398</td>\n",
       "      <td id=\"T_1f5b3_row2_col2\" class=\"data row2 col2\" >465</td>\n",
       "      <td id=\"T_1f5b3_row2_col3\" class=\"data row2 col3\" >13.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1f5b3_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_1f5b3_row3_col1\" class=\"data row3 col1\" >26639</td>\n",
       "      <td id=\"T_1f5b3_row3_col2\" class=\"data row3 col2\" >1729</td>\n",
       "      <td id=\"T_1f5b3_row3_col3\" class=\"data row3 col3\" >6.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1f5b3_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_1f5b3_row4_col1\" class=\"data row4 col1\" >8917</td>\n",
       "      <td id=\"T_1f5b3_row4_col2\" class=\"data row4 col2\" >341</td>\n",
       "      <td id=\"T_1f5b3_row4_col3\" class=\"data row4 col3\" >3.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1f5b3_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_1f5b3_row5_col1\" class=\"data row5 col1\" >3759</td>\n",
       "      <td id=\"T_1f5b3_row5_col2\" class=\"data row5 col2\" >147</td>\n",
       "      <td id=\"T_1f5b3_row5_col3\" class=\"data row5 col3\" >3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1f5b3_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_1f5b3_row6_col1\" class=\"data row6 col1\" >902</td>\n",
       "      <td id=\"T_1f5b3_row6_col2\" class=\"data row6 col2\" >28</td>\n",
       "      <td id=\"T_1f5b3_row6_col3\" class=\"data row6 col3\" >3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1f5b3_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_1f5b3_row7_col1\" class=\"data row7 col1\" >456</td>\n",
       "      <td id=\"T_1f5b3_row7_col2\" class=\"data row7 col2\" >37</td>\n",
       "      <td id=\"T_1f5b3_row7_col3\" class=\"data row7 col3\" >8.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_1f5b3_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_1f5b3_row8_col1\" class=\"data row8 col1\" >147</td>\n",
       "      <td id=\"T_1f5b3_row8_col2\" class=\"data row8 col2\" >8</td>\n",
       "      <td id=\"T_1f5b3_row8_col3\" class=\"data row8 col3\" >5.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5b3_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_1f5b3_row9_col0\" class=\"data row9 col0\" >9</td>\n",
       "      <td id=\"T_1f5b3_row9_col1\" class=\"data row9 col1\" >171</td>\n",
       "      <td id=\"T_1f5b3_row9_col2\" class=\"data row9 col2\" >14</td>\n",
       "      <td id=\"T_1f5b3_row9_col3\" class=\"data row9 col3\" >8.2%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb3f0361c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and percentage of missing values grouped by age-binned: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_37de9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_37de9_level0_col0\" class=\"col_heading level0 col0\" >age-binned</th>\n",
       "      <th id=\"T_37de9_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_37de9_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_37de9_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_37de9_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_37de9_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_37de9_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_37de9_row0_col3\" class=\"data row0 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_37de9_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_37de9_row1_col1\" class=\"data row1 col1\" >3623</td>\n",
       "      <td id=\"T_37de9_row1_col2\" class=\"data row1 col2\" >655</td>\n",
       "      <td id=\"T_37de9_row1_col3\" class=\"data row1 col3\" >18.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_37de9_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_37de9_row2_col1\" class=\"data row2 col1\" >12170</td>\n",
       "      <td id=\"T_37de9_row2_col2\" class=\"data row2 col2\" >878</td>\n",
       "      <td id=\"T_37de9_row2_col3\" class=\"data row2 col3\" >7.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_37de9_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_37de9_row3_col1\" class=\"data row3 col1\" >12838</td>\n",
       "      <td id=\"T_37de9_row3_col2\" class=\"data row3 col2\" >547</td>\n",
       "      <td id=\"T_37de9_row3_col3\" class=\"data row3 col3\" >4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_37de9_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_37de9_row4_col1\" class=\"data row4 col1\" >10403</td>\n",
       "      <td id=\"T_37de9_row4_col2\" class=\"data row4 col2\" >413</td>\n",
       "      <td id=\"T_37de9_row4_col3\" class=\"data row4 col3\" >4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_37de9_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_37de9_row5_col1\" class=\"data row5 col1\" >6202</td>\n",
       "      <td id=\"T_37de9_row5_col2\" class=\"data row5 col2\" >369</td>\n",
       "      <td id=\"T_37de9_row5_col3\" class=\"data row5 col3\" >5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_37de9_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_37de9_row6_col1\" class=\"data row6 col1\" >2738</td>\n",
       "      <td id=\"T_37de9_row6_col2\" class=\"data row6 col2\" >526</td>\n",
       "      <td id=\"T_37de9_row6_col3\" class=\"data row6 col3\" >19.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_37de9_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_37de9_row7_col1\" class=\"data row7 col1\" >720</td>\n",
       "      <td id=\"T_37de9_row7_col2\" class=\"data row7 col2\" >198</td>\n",
       "      <td id=\"T_37de9_row7_col3\" class=\"data row7 col3\" >27.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_37de9_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_37de9_row8_col1\" class=\"data row8 col1\" >148</td>\n",
       "      <td id=\"T_37de9_row8_col2\" class=\"data row8 col2\" >34</td>\n",
       "      <td id=\"T_37de9_row8_col3\" class=\"data row8 col3\" >23.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_37de9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_37de9_row9_col0\" class=\"data row9 col0\" >9</td>\n",
       "      <td id=\"T_37de9_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_37de9_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_37de9_row9_col3\" class=\"data row9 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb45d5fac0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and percentage of missing values grouped by sex-num: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1ae8d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1ae8d_level0_col0\" class=\"col_heading level0 col0\" >sex-num</th>\n",
       "      <th id=\"T_1ae8d_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_1ae8d_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_1ae8d_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae8d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1ae8d_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_1ae8d_row0_col1\" class=\"data row0 col1\" >32650</td>\n",
       "      <td id=\"T_1ae8d_row0_col2\" class=\"data row0 col2\" >2123</td>\n",
       "      <td id=\"T_1ae8d_row0_col3\" class=\"data row0 col3\" >6.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ae8d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1ae8d_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_1ae8d_row1_col1\" class=\"data row1 col1\" >16192</td>\n",
       "      <td id=\"T_1ae8d_row1_col2\" class=\"data row1 col2\" >1497</td>\n",
       "      <td id=\"T_1ae8d_row1_col3\" class=\"data row1 col3\" >9.2%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb40c3e5e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and percentage of missing values grouped by capital-gain-binned: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3d610\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3d610_level0_col0\" class=\"col_heading level0 col0\" >capital-gain-binned</th>\n",
       "      <th id=\"T_3d610_level0_col1\" class=\"col_heading level0 col1\" >total_count</th>\n",
       "      <th id=\"T_3d610_level0_col2\" class=\"col_heading level0 col2\" >missing_values_count</th>\n",
       "      <th id=\"T_3d610_level0_col3\" class=\"col_heading level0 col3\" >percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3d610_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_3d610_row0_col1\" class=\"data row0 col1\" >47708</td>\n",
       "      <td id=\"T_3d610_row0_col2\" class=\"data row0 col2\" >3572</td>\n",
       "      <td id=\"T_3d610_row0_col3\" class=\"data row0 col3\" >7.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3d610_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_3d610_row1_col1\" class=\"data row1 col1\" >753</td>\n",
       "      <td id=\"T_3d610_row1_col2\" class=\"data row1 col2\" >22</td>\n",
       "      <td id=\"T_3d610_row1_col3\" class=\"data row1 col3\" >2.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3d610_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_3d610_row2_col1\" class=\"data row2 col1\" >128</td>\n",
       "      <td id=\"T_3d610_row2_col2\" class=\"data row2 col2\" >9</td>\n",
       "      <td id=\"T_3d610_row2_col3\" class=\"data row2 col3\" >7.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3d610_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_3d610_row3_col1\" class=\"data row3 col1\" >6</td>\n",
       "      <td id=\"T_3d610_row3_col2\" class=\"data row3 col2\" >2</td>\n",
       "      <td id=\"T_3d610_row3_col3\" class=\"data row3 col3\" >33.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3d610_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_3d610_row4_col1\" class=\"data row4 col1\" >3</td>\n",
       "      <td id=\"T_3d610_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_3d610_row4_col3\" class=\"data row4 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3d610_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_3d610_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_3d610_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_3d610_row5_col3\" class=\"data row5 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3d610_row6_col0\" class=\"data row6 col0\" >6</td>\n",
       "      <td id=\"T_3d610_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_3d610_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_3d610_row6_col3\" class=\"data row6 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3d610_row7_col0\" class=\"data row7 col0\" >7</td>\n",
       "      <td id=\"T_3d610_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_3d610_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_3d610_row7_col3\" class=\"data row7 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3d610_row8_col0\" class=\"data row8 col0\" >8</td>\n",
       "      <td id=\"T_3d610_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_3d610_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_3d610_row8_col3\" class=\"data row8 col3\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3d610_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3d610_row9_col0\" class=\"data row9 col0\" >9</td>\n",
       "      <td id=\"T_3d610_row9_col1\" class=\"data row9 col1\" >244</td>\n",
       "      <td id=\"T_3d610_row9_col2\" class=\"data row9 col2\" >15</td>\n",
       "      <td id=\"T_3d610_row9_col3\" class=\"data row9 col3\" >6.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdb45d5f970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add column to label if row has ? in any column\n",
    "ds_combined['has_missing_values'] = ds_combined.apply(lambda x: 1 if '?' in x.values else 0, axis=1)\n",
    "\n",
    "# Print percentage of rows with ?\n",
    "print(\"Percentage of rows with missing values: \", ds_combined['has_missing_values'].sum()/ds_combined.shape[0] * 100, \"%\")\n",
    "\n",
    "# Print percentage of rows with only ? in native-country column to total rows with ?\n",
    "print(\"Percentage of rows with missing values in native-country column to total rows with missing values: \", ds_combined[(ds_combined['has_missing_values'] == 1) & (ds_combined['native-country'] == '?') & (ds_combined['workclass'] != '?') & (ds_combined['occupation'] != '?')].shape[0]\n",
    "      / ds_combined[ds_combined['has_missing_values'] == 1].shape[0] * 100, \"%\")\n",
    "\n",
    "# Print percentage of rows with only ? in native-country column to total rows\n",
    "print(\"Percentage of rows with missing values in native-country column to total rows: \", ds_combined[(ds_combined['native-country'] == '?') & (ds_combined['workclass'] != '?') & (ds_combined['occupation'] != '?')].shape[0]\n",
    "        / ds_combined.shape[0] * 100, \"%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Function to generate count of ? grouped by specified column\n",
    "def generate_missing_values_count(dataset, group_by_column_name):\n",
    "    # Generate total count of records by group\n",
    "    dataset_missing_values = dataset.groupby(group_by_column_name).size().reset_index(name='total_count')\n",
    "    # Initialize missing_values_count column with 0\n",
    "    dataset_missing_values['missing_values_count'] = 0\n",
    "    # Initialize mssing_value_percentage column with 0\n",
    "    dataset_missing_values['percentage'] = 0\n",
    "\n",
    "    # Generate count and percentage of ? in each group\n",
    "    for index, row in dataset_missing_values.iterrows():\n",
    "        if row['total_count'] > 0:\n",
    "            dataset_missing_values.loc[index, 'missing_values_count'] = dataset[(dataset[group_by_column_name] == row[group_by_column_name]) & (dataset['has_missing_values'] == 1)].shape[0]\n",
    "            dataset_missing_values.loc[index, 'percentage'] = dataset_missing_values.loc[index, 'missing_values_count']/dataset_missing_values.loc[index, 'total_count'] * 100\n",
    "            \n",
    "    return dataset_missing_values\n",
    "\n",
    "# Print missing value count and percentage grouped by imbalance columns in formatted table\n",
    "for i in imbalanced_columns + feature_correlation.drop('income-num').abs().sort_values(ascending=False).head(5).index.tolist():\n",
    "    temp_missing_values_count = generate_missing_values_count(ds_combined, i)\n",
    "    print(f\"Count and percentage of missing values grouped by {i}: \")\n",
    "    temp_missing_values_count.style.set_table_styles([{'selector' : '',\n",
    "        'props' : [('border', '2px solid green')]}])\n",
    "    display(temp_missing_values_count.style.format({'percentage': '{:.1f}%'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion and conclusion on approach for missing values - Imputation vs Removal\n",
    "\n",
    "From data exploration analysis, the missing values (coded with ?) are at random. It can be argued that consideration of removing all records/ observations with at least a missing value will reduce the data size by 7.411653904426519 % which is considerably a huge set. In addition, deeper analysis performed to measure the distribution of the missing value by features with imbalanced data (income, race, native-country) and significant features (education, hours-per-week, age, gendar) suggest that the missing values are not evenly distributed (such as more missing values in smaller sized categories/ groups). Thus, removing missing values can further create bias in the dataset.\n",
    "\n",
    "However, it cannot be ascertained if the dataset imbalance is a result of the natural phenomena and demogrphic representation (e.g. education, age, gender, workclass, occupation). There are several approach to imputation of data to fix missing values but requires deeper understanding of why the data may be missing (e.g. lack of suitable choice for selection during survey) and the source/ environment to provide greater confidence in the reliability of the selected approach. A general approach to \"patching\" missing values may equally introduce bias and errors.\n",
    "\n",
    "Dropping low significance features or deleting records with missing values can provide for dimensionality and numerousity reduction. For example, native-country feature has a correlation measure of 1.6793% to income and native-country only missing value contributes to 22.40331491712707 % of total missing values, which means these records can be retained by dropping this column (reducing the % of columns to be deleted by 1.660456164776217 % from 7.411653904426519 %). There are also benefits to such reductions which includes less misleading data, better accuracy, integrity, and performance.\n",
    "\n",
    "As a conclusion, with the primary focus/ attention on the development of the classification algorithm, simple removal approach by deleting instances with missing values in any of the columns will be adopted supported the discussion in this section.\n",
    "> Note: Decision algorithm types C5.0 and CART are able to deal missing values (Emmanuel et al., 2021; Hambali et al., 2019; ‘Decision Tree’, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d8/3t9hsthx62q3f_xw0yrv_qvw0000gn/T/ipykernel_94272/52596467.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds_combined_clean.drop(columns=['has_missing_values'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing values\n",
    "ds_combined_clean = ds_combined[ds_combined['has_missing_values'] == 0]\n",
    "\n",
    "# has_missing_values column is no longer required\n",
    "ds_combined_clean.drop(columns=['has_missing_values'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling/ Normalization/ Standardization and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined dataset with no missing values:  (45222, 17) \n",
      "\n",
      "Columns in ds_combined_no_missing_values:  ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'dataset', 'occupation-num', 'income-num', 'sex-num', 'race-num', 'workclass-num', 'native-country-num', 'relationship-num', 'age-binned', 'hours-per-week-binned', 'capital-gain-binned', 'capital-loss-binned'] \n",
      "\n",
      "   age  education-num  capital-gain  capital-loss  hours-per-week dataset  \\\n",
      "0   39             13          2174             0              40   train   \n",
      "1   50             13             0             0              13   train   \n",
      "2   38              9             0             0              40   train   \n",
      "3   53              7             0             0              40   train   \n",
      "4   28             13             0             0              40   train   \n",
      "\n",
      "   occupation-num  income-num  sex-num  race-num  workclass-num  \\\n",
      "0               1           0        0         1              4   \n",
      "1               4           0        0         1              2   \n",
      "2               6           0        0         1              1   \n",
      "3               6           0        0         0              1   \n",
      "4              10           0        1         0              1   \n",
      "\n",
      "   native-country-num  relationship-num age-binned hours-per-week-binned  \\\n",
      "0                   1                 0          3                     3   \n",
      "1                   1                 1          4                     1   \n",
      "2                   1                 0          3                     3   \n",
      "3                   1                 1          5                     3   \n",
      "4                   9                 4          2                     3   \n",
      "\n",
      "  capital-gain-binned capital-loss-binned  \n",
      "0                   0                   0  \n",
      "1                   0                   0  \n",
      "2                   0                   0  \n",
      "3                   0                   0  \n",
      "4                   0                   0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d8/3t9hsthx62q3f_xw0yrv_qvw0000gn/T/ipykernel_94272/242417604.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds_combined_clean.drop(columns=columns_to_drop, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns not in required_columns and keeping dataset column for splitting train/ test later\n",
    "columns_to_drop = set(ds_combined_clean.columns) - required_columns - set(['dataset'])\n",
    "\n",
    "# Drop insignificant features\n",
    "columns_to_drop = columns_to_drop.union(set(insignificant_features))\n",
    "\n",
    "# Drop columns\n",
    "ds_combined_clean.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Print shape and data of ds_combined_no_missing_values\n",
    "print(\"Shape of the combined dataset with no missing values: \", str(ds_combined_clean.shape), \"\\n\")\n",
    "\n",
    "# List columns in ds_combined_no_missing_values\n",
    "print(\"Columns in ds_combined_no_missing_values: \", ds_combined_clean.columns.tolist(), \"\\n\")\n",
    "\n",
    "# Print data of ds_combined_no_missing_values\n",
    "print(ds_combined_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling/ Normalization/ Standardization will not be applied as it is not required for the algorithm to be developed. In addtion, we may not have sufficient infomraiton about the distrubtion and variety/ veracity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other possible classifcation algorithm includes Logistic Regression, Decision Trees, Random Forests, Gradient Boosting, Support Vector Machines, K-Nearest Neighbors, Naive Bayes, Neural Networks. However, with a dataset with relatively small number of features, it is ideal to start with simpler models such as Logistic Regression or Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree has attributes and strengths that makes it suitable for our use case. It is easy to interpret and understand, which also comes with tools that allows us to plot/ visualize the structure/ flow especially when the dimension of features is small. In addtiion, it has implicit ability to perform feature selection by prioritizing the important features at the top of the tree. It is also able to capture non-linear relationships between features of variable data types (categorical and numerical) and the target without requiring extensive data preprocessing which makes it applicable for our census income dataset.\n",
    "\n",
    "However, there are weaknesses that needs to be taken note of. Firstly, deicision trees are commonly prone to over-fitting especially when the tree becomes too deep (although there are techniques to address this). Second, it is sensitive to variations in data which makes it highly unstable with data udpates. Lastly, it is biased towards majority class and thus can perform poorly with classes that are under-represented - which is a case with our dataset for income groups, race, native-country, education, workclass, age, hours-per-week, and gender which are likely a natural representation and is common in real world dataset. This can be addressed with certain strategies such as sampling, class weights, or k-fold cross validation for the case of this task.\n",
    "\n",
    "From the correlation matrix, race and native-country have lower significance (<5%) compared to other features.\n",
    "- education-num            0.332613\n",
    "- hours-per-week-binned    0.226595\n",
    "- age-binned               0.224554\n",
    "- sex-num                  0.214628\n",
    "- capital-gain-binned      0.177530\n",
    "- workclass-num            0.154423\n",
    "- capital-loss-binned      0.121946\n",
    "- occupation-num           0.076722\n",
    "- relationship-num         0.054008\n",
    "- race-num                 0.033723\n",
    "- native-country-num       0.016793\n",
    "- fnlwgt                   0.006339\n",
    "- marital-status-num       0.002517\n",
    "\n",
    "The different types of decision tree algorithm includes ID3, C4.5, C5.0, and CART.\n",
    "> Information about CART. CART can process both categorical and continuous attributes, and can handle missing values. It uses the Gini Index as attribute selection measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build decision tree algorithem code/ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "Required"
    ]
   },
   "outputs": [],
   "source": [
    "# Python code for CART (Classification and Regression Trees) decision tree algorithm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Decision trees are constructed recursively and linked by nodes - Root node, Internal node, Leaf node, and branches.\n",
    "# Left and right branches are used to represent the decision tree\n",
    "\n",
    "# Create class Node to represent each node in the decision tree\n",
    "class Node:\n",
    "    # Initialize Node class with data/ target, left/ right attributes, feature, threshold, value attributes\n",
    "    def __init__(self, data, target, feature=None, threshold=None, value=None):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "\n",
    "# Create the CART decision tree class\n",
    "class CARTDecisionTree:\n",
    "    \n",
    "    # Initialize CARTDecisionTree class with max_depth, min_size, and root attributes\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    # Function for calculating the Gini index\n",
    "    def gini(self, target):\n",
    "        # Size of the target column\n",
    "        m = target.shape[0]\n",
    "\n",
    "        # Get the unique values in the target column\n",
    "        unique_labels = np.unique(target)\n",
    "\n",
    "        # Iterate through each unique value in the target column, calculate the probability of each unique value\n",
    "        gini = 1 - sum([(np.sum(target == label)/m)**2 for label in unique_labels])\n",
    "\n",
    "        return gini\n",
    "    \n",
    "    # Function for splitting the data into two groups\n",
    "    def split_data(self, data, target, feature_idx, threshold):\n",
    "        # Get the feature column\n",
    "        feature = data[:, feature_idx]\n",
    "\n",
    "        # Split the data into two groups\n",
    "        left_data = data[feature < threshold]\n",
    "        right_data = data[feature >= threshold]\n",
    "\n",
    "        # Split the target into two groups\n",
    "        left_target = target[feature < threshold]\n",
    "        right_target = target[feature >= threshold]\n",
    "\n",
    "        return left_data, right_data, left_target, right_target\n",
    "\n",
    "    # Function for finding the best split\n",
    "    def best_split(self, data, target):\n",
    "        # Get the shape of the data\n",
    "        sample_count, feature_count = data.shape\n",
    "\n",
    "        best_split = {'feature_idx': None, 'threshold': None, 'gini': 1}\n",
    "\n",
    "        # If there is only one row, return None\n",
    "        if sample_count <= 1:\n",
    "            return best_split\n",
    "        \n",
    "        # Calculate the Gini index for the parent node\n",
    "        parent_gini = self.gini(target)\n",
    "        best_gini = 1\n",
    "\n",
    "        # Iterate through each feature\n",
    "        for feature_idx in range(feature_count):\n",
    "            # Get the unique values in the feature column\n",
    "            unique_values = np.unique(data[:, feature_idx])\n",
    "\n",
    "            # Iterate through each unique value in the feature column\n",
    "            for threshold in unique_values:\n",
    "                # Split the data into two groups\n",
    "                left_data, right_data, left_target, right_target = self.split_data(data, target, feature_idx, threshold)\n",
    "\n",
    "                # Skip the split if it does not divide the dataset\n",
    "                if len(left_data) == 0 or len(right_data) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate the Gini index for each group\n",
    "                left_gini = self.gini(left_target)\n",
    "                right_gini = self.gini(right_target)\n",
    "\n",
    "                # Calculate the weighted Gini index\n",
    "                weighted_gini = (len(left_data)/sample_count) * left_gini + (len(right_data)/sample_count) * right_gini\n",
    "\n",
    "                # If the weighted Gini index is better than the current best, update the best split\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_split = {'feature_idx': feature_idx, 'threshold': threshold, 'gini': weighted_gini}\n",
    "                    best_gini = weighted_gini\n",
    "\n",
    "        # If the best split does not reduce the Gini index, return None\n",
    "        if best_split['gini'] >= parent_gini:\n",
    "            return {'feature_idx': None, 'threshold': None, 'gini': parent_gini}\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    # Function to get the most common label in the target column\n",
    "    def most_common_label(self, target):\n",
    "        # Get the unique values in the target column\n",
    "        unique_labels, counts_unique_labels = np.unique(target, return_counts=True)\n",
    "\n",
    "        # Get the index of the most common label\n",
    "        most_common_label_idx = np.argmax(counts_unique_labels)\n",
    "\n",
    "        return unique_labels[most_common_label_idx]\n",
    "\n",
    "    # Function to fit/ train the CART decision tree\n",
    "    def fit(self, data, target):\n",
    "        self.root = self.grow_tree(data, target)\n",
    "\n",
    "    # Function to build the CART decision tree\n",
    "    def grow_tree(self, data, target, depth=0):        \n",
    "        # Get the unique values in the target column\n",
    "        unique_labels = np.unique(target)\n",
    "\n",
    "        # If the number of unique values in the target column is 1 or maximum depth is reached, return the unique value\n",
    "        if (len(unique_labels) == 1) or (self.max_depth and depth >= self.max_depth):\n",
    "            leaf_value = self.most_common_label(target)\n",
    "            return Node(data, target, value=leaf_value)\n",
    "        \n",
    "        # Get the best split\n",
    "        best_split = self.best_split(data, target)\n",
    "\n",
    "        # If the best split is None or exceeds max_depth, create a leaf node\n",
    "        # Otherwise, create a branch node with the returned feature index and threshold in the best split\n",
    "        if best_split['feature_idx'] == None or (self.max_depth and depth >= self.max_depth):\n",
    "            leaf_value = self.most_common_label(target)\n",
    "            return Node(data, target, value=leaf_value)\n",
    "        else:\n",
    "            left_data, right_data, left_target, right_target = self.split_data(data, target, best_split['feature_idx'], best_split['threshold'])\n",
    "            depth += 1\n",
    "            node = Node(data, target, feature=best_split['feature_idx'], threshold=best_split['threshold'])\n",
    "            node.left = self.grow_tree(left_data, left_target, depth)\n",
    "            node.right = self.grow_tree(right_data, right_target, depth)\n",
    "            return node\n",
    "        \n",
    "    # Function to predict the target column\n",
    "    def predict(self, data):\n",
    "        return np.array([self.predict_row(row) for row in data])   \n",
    "    \n",
    "    # Function to predict the target column for a row\n",
    "    def predict_row(self, row):\n",
    "        node = self.root\n",
    "        while node.left:\n",
    "            if row[node.feature] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n",
    "        \n",
    "    # Function to print the CART decision tree\n",
    "    def print_tree(self, node, depth=0, feature_names=None):\n",
    "        if node.left:\n",
    "            if feature_names:\n",
    "                feature_name = \"(\" + feature_names[node.feature] + \")\"\n",
    "                print('%s[X%d%s < %.3f]' % ((depth*' ', (node.feature+1), feature_name, node.threshold)))\n",
    "            else:\n",
    "                print('%s[X%d < %.3f]' % ((depth*' ', (node.feature+1), node.threshold)))            \n",
    "            self.print_tree(node.left, depth+1, feature_names)\n",
    "            self.print_tree(node.right, depth+1, feature_names)\n",
    "        else:\n",
    "            print('%s[%s]' % ((depth*' ', node.value)))\n",
    "\n",
    "    # Function to calculate the accuracy of the model\n",
    "    def accuracy(self, y_act, y_pred):\n",
    "        accuracy = np.sum(y_act == y_pred) / len(y_act)\n",
    "        return accuracy\n",
    "    \n",
    "    # Function to calculate the precision of the model\n",
    "    def precision(self, y_act, y_pred):\n",
    "        # Get the unique values in the target column\n",
    "        unique_labels = np.unique(y_act)\n",
    "\n",
    "        # Initialize the precision score\n",
    "        precision_score = 0\n",
    "\n",
    "        # Iterate through each unique value in the target column\n",
    "        for label in unique_labels:\n",
    "            # Get the indices of the unique value in the target column\n",
    "            indices = np.where(y_pred == label)[0]\n",
    "\n",
    "            # Calculate the true positives\n",
    "            true_positives = np.sum(y_act[indices] == label)\n",
    "\n",
    "            # Calculate the precision for the unique value\n",
    "            precision = true_positives / len(indices)\n",
    "\n",
    "            # Add the precision to the precision score\n",
    "            precision_score += precision\n",
    "\n",
    "        # Calculate the average precision score\n",
    "        precision_score /= len(unique_labels)\n",
    "\n",
    "        return precision_score\n",
    "    \n",
    "    # Function to calculate the recall of the model\n",
    "    def recall(self, y_act, y_pred):\n",
    "        # Get the unique values in the target column\n",
    "        unique_labels = np.unique(y_act)\n",
    "\n",
    "        # Initialize the recall score\n",
    "        recall_score = 0\n",
    "\n",
    "        # Iterate through each unique value in the target column\n",
    "        for label in unique_labels:\n",
    "            # Get the indices of the unique value in the target column\n",
    "            indices = np.where(y_act == label)[0]\n",
    "\n",
    "            # Calculate the true positives\n",
    "            true_positives = np.sum(y_pred[indices] == label)\n",
    "\n",
    "            # Calculate the recall for the unique value\n",
    "            recall = true_positives / len(indices)\n",
    "\n",
    "            # Add the recall to the recall score\n",
    "            recall_score += recall\n",
    "\n",
    "        # Calculate the average recall score\n",
    "        recall_score /= len(unique_labels)\n",
    "\n",
    "        return recall_score\n",
    "    \n",
    "    # Function to calculate the F1 score of the model\n",
    "    def f1_score(self, y_act, y_pred):\n",
    "        # Calculate the precision and recall of the model\n",
    "        precision = self.precision(y_act, y_pred)\n",
    "        recall = self.recall(y_act, y_pred)\n",
    "\n",
    "        # Calculate the F1 score\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        return f1_score\n",
    "    \n",
    "    # Function to calculate the confusion matrix of the model\n",
    "    def confusion_matrix(self, y_act, y_pred):\n",
    "        # Get the unique values in the target column\n",
    "        unique_labels = np.unique(y_act)\n",
    "\n",
    "        # Initialize the confusion matrix\n",
    "        confusion_matrix = np.zeros((len(unique_labels), len(unique_labels)), dtype=int)\n",
    "\n",
    "        # Iterate through each unique value in the target column\n",
    "        for i in range(len(unique_labels)):\n",
    "            # Get the indices of the unique value in the target column\n",
    "            indices = np.where(y_act == unique_labels[i])[0]\n",
    "\n",
    "            # Iterate through each unique value in the target column\n",
    "            for j in range(len(unique_labels)):\n",
    "                # Calculate the true positives\n",
    "                true_positives = np.sum(y_pred[indices] == unique_labels[j])\n",
    "\n",
    "                # Add the true positives to the confusion matrix\n",
    "                confusion_matrix[i][j] = true_positives\n",
    "\n",
    "        return confusion_matrix\n",
    "    \n",
    "    # Function to perform cross validation with k folds\n",
    "    def cross_validation(self, data, target, k=5):\n",
    "        # Get the shape of the data\n",
    "        sample_count, feature_count = data.shape\n",
    "\n",
    "        # Initialize the accuracy, precision, recall, F1 score, and confusion matrix\n",
    "        accuracy = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1_score = 0\n",
    "        confusion_matrix = np.zeros((2, 2), dtype=float)\n",
    "\n",
    "        # Split the data into k folds\n",
    "        # folds = np.array_split(data, k)\n",
    "        # targets = np.array_split(target, k)\n",
    "        # Calculate the number of samples in each fold\n",
    "        samples_per_fold = len(data) // k\n",
    "\n",
    "        # Split data and target into k folds with equal sizes\n",
    "        folds = [data[i * samples_per_fold:(i + 1) * samples_per_fold] for i in range(k)]\n",
    "        targets = [target[i * samples_per_fold:(i + 1) * samples_per_fold] for i in range(k)]\n",
    "\n",
    "        # Iterate through each fold measuring training and predict time\n",
    "        training_time = 0\n",
    "        predict_time = 0\n",
    "\n",
    "        for i in range(k):\n",
    "            # Get the training and testing data\n",
    "            train_data = np.concatenate(np.delete(folds, i, axis=0))\n",
    "            train_target = np.concatenate(np.delete(targets, i, axis=0))\n",
    "            test_data = folds[i]\n",
    "            test_target = targets[i]\n",
    "\n",
    "            # Fit the model and measure training time\n",
    "            start_time = time.time()\n",
    "            self.fit(train_data, train_target)\n",
    "            training_time += time.time() - start_time\n",
    "\n",
    "            # Predict the target column and measure predict time\n",
    "            start_time = time.time()\n",
    "            y_pred = self.predict(test_data)\n",
    "            predict_time += time.time() - start_time\n",
    "\n",
    "            # Calculate the accuracy, precision, recall, F1 score, and confusion matrix\n",
    "            accuracy += self.accuracy(test_target, y_pred)\n",
    "            precision += self.precision(test_target, y_pred)\n",
    "            recall += self.recall(test_target, y_pred)\n",
    "            f1_score += self.f1_score(test_target, y_pred)\n",
    "            confusion_matrix += self.confusion_matrix(test_target, y_pred)\n",
    "\n",
    "        # Calculate the average accuracy, precision, recall, F1 score, and confusion matrix\n",
    "        accuracy /= k\n",
    "        precision /= k\n",
    "        recall /= k\n",
    "        f1_score /= k\n",
    "        confusion_matrix /= k\n",
    "\n",
    "        return accuracy, precision, recall, f1_score, confusion_matrix, training_time, predict_time\n",
    "    \n",
    "    # Reset the CART decision tree\n",
    "    def reset(self):\n",
    "        self.root = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Earlier cells in this notebook must be executed before proceeding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test algorithm with binned features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ds_combined_clean:  ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'dataset', 'occupation-num', 'income-num', 'sex-num', 'race-num', 'workclass-num', 'native-country-num', 'relationship-num', 'age-binned', 'hours-per-week-binned', 'capital-gain-binned', 'capital-loss-binned'] \n",
      "\n",
      "Columns selected with ds_header_names_encoded:  ['workclass-num', 'occupation-num', 'relationship-num', 'capital-loss-binned', 'race-num', 'native-country-num', 'income-num', 'education-num', 'age-binned', 'hours-per-week-binned', 'sex-num', 'capital-gain-binned'] \n",
      "\n",
      "Testing CART decision tree with encoded columns using binned values\n",
      "==============================================\n",
      "Shape of train_data:  (30162, 11)\n",
      "Shape of train_target:  (30162,)\n",
      "Shape of test_data:  (15060, 11)\n",
      "Shape of test_target:  (15060,)\n",
      "Variable type of train_data:  <class 'pandas.core.frame.DataFrame'>\n",
      "Variable type of train_target:  <class 'pandas.core.series.Series'>\n",
      "Variable type of test_data:  <class 'pandas.core.frame.DataFrame'>\n",
      "Variable type of test_target:  <class 'pandas.core.series.Series'> \n",
      "\n",
      "Count of predicted values:\n",
      "(array([0, 1]), array([11725,  3335])) \n",
      "\n",
      "Accuracy:  0.8069721115537849\n",
      "Precision:  0.7396798189391445\n",
      "Recall:  0.7229770175104682\n",
      "F1 score:  0.7312330494518263\n",
      "Confusion matrix: \n",
      " [[10089  1271]\n",
      " [ 1636  2064]]\n",
      "Error rate:  0.1930278884462151 \n",
      "\n",
      "Training time:  8.176728963851929\n",
      "Prediction time:  0.10496377944946289\n"
     ]
    }
   ],
   "source": [
    "# Print the columns in ds_combined_clean\n",
    "print(\"Columns in ds_combined_clean: \", ds_combined_clean.columns.tolist(), \"\\n\")\n",
    "\n",
    "# Print columns selected with ds_header_names_encoded\n",
    "features_selected = list(set(ds_combined_clean.columns).intersection(set(ds_header_names_encoded)))\n",
    "print(\"Columns selected with ds_header_names_encoded: \", features_selected, \"\\n\")\n",
    "\n",
    "# Test the CART decision tree with encoded columns with binned values\n",
    "print(\"Testing CART decision tree with encoded columns using binned values\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "# Split the data into train and test, selecting only the encoded columns\n",
    "train_data = ds_combined_clean[ds_combined_clean['dataset'] == 'train'].drop(columns=['dataset'])\n",
    "train_data = train_data[features_selected].drop(columns=['income-num'])\n",
    "train_target = ds_combined_clean[ds_combined_clean['dataset'] == 'train']['income-num']\n",
    "test_data = ds_combined_clean[ds_combined_clean['dataset'] == 'test'].drop(columns=['dataset'])\n",
    "test_data = test_data[features_selected].drop(columns=['income-num'])\n",
    "test_target = ds_combined_clean[ds_combined_clean['dataset'] == 'test']['income-num']\n",
    "\n",
    "# Print the shape of the train and test data, train and test target, and variable type\n",
    "print(\"Shape of train_data: \", train_data.shape)\n",
    "print(\"Shape of train_target: \", train_target.shape)\n",
    "print(\"Shape of test_data: \", test_data.shape)\n",
    "print(\"Shape of test_target: \", test_target.shape)\n",
    "print(\"Variable type of train_data: \", type(train_data))\n",
    "print(\"Variable type of train_target: \", type(train_target))\n",
    "print(\"Variable type of test_data: \", type(test_data))\n",
    "print(\"Variable type of test_target: \", type(test_target), \"\\n\")\n",
    "\n",
    "# Initialize the CART decision tree\n",
    "cart_decision_tree = CARTDecisionTree()\n",
    "\n",
    "# Fit the CART decision tree and measure time taken\n",
    "start_time = time.time()\n",
    "cart_decision_tree.fit(train_data.to_numpy(), train_target.to_numpy())\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Print the CART decision tree\n",
    "# print(\"CART decision tree:\")\n",
    "# cart_decision_tree.print_tree(cart_decision_tree.root, feature_names=train_data.columns.tolist())\n",
    "\n",
    "# Predict the target column and meaure time taken\n",
    "start_time = time.time()\n",
    "y_pred = cart_decision_tree.predict(test_data.to_numpy())\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Print count of predicted values\n",
    "print(\"Count of predicted values:\")\n",
    "print(np.unique(y_pred, return_counts=True), \"\\n\")\n",
    "\n",
    "# Calculate the accuracy, precision, recall, F1 score, and confusion matrix\n",
    "accuracy = cart_decision_tree.accuracy(test_target.to_numpy(), y_pred)\n",
    "precision = cart_decision_tree.precision(test_target.to_numpy(), y_pred)\n",
    "recall = cart_decision_tree.recall(test_target.to_numpy(), y_pred)\n",
    "f1_score = cart_decision_tree.f1_score(test_target.to_numpy(), y_pred)\n",
    "confusion_matrix = cart_decision_tree.confusion_matrix(test_target.to_numpy(), y_pred)\n",
    "\n",
    "# Print the accuracy, precision, recall, F1 score, and confusion matrix\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1_score)\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix)\n",
    "print(\"Error rate: \", 1 - accuracy, \"\\n\")\n",
    "\n",
    "# Print the training and prediction time\n",
    "print(\"Training time: \", training_time)\n",
    "print(\"Prediction time: \", prediction_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test algorithm with continuous values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ds_combined_clean:  ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'dataset', 'occupation-num', 'income-num', 'sex-num', 'race-num', 'workclass-num', 'native-country-num', 'relationship-num', 'age-binned', 'hours-per-week-binned', 'capital-gain-binned', 'capital-loss-binned'] \n",
      "\n",
      "Columns selected with ds_header_names_encoded:  ['workclass-num', 'occupation-num', 'relationship-num', 'race-num', 'capital-gain', 'native-country-num', 'income-num', 'capital-loss', 'education-num', 'hours-per-week', 'age', 'sex-num'] \n",
      "\n",
      "Testing CART decision tree with encoded columns using continuous values\n",
      "==============================================\n",
      "Shape of train_data:  (30162, 11)\n",
      "Shape of train_target:  (30162,)\n",
      "Shape of test_data:  (15060, 11)\n",
      "Shape of test_target:  (15060,)\n",
      "Variable type of train_data:  <class 'pandas.core.frame.DataFrame'>\n",
      "Variable type of train_target:  <class 'pandas.core.series.Series'>\n",
      "Variable type of test_data:  <class 'pandas.core.frame.DataFrame'>\n",
      "Variable type of test_target:  <class 'pandas.core.series.Series'> \n",
      "\n",
      "Count of predicted values:\n",
      "(array([0, 1]), array([11472,  3588])) \n",
      "\n",
      "Accuracy:  0.8115537848605577\n",
      "Precision:  0.7457743559424022\n",
      "Recall:  0.740684478492577\n",
      "F1 score:  0.7432207029322856\n",
      "Confusion matrix: \n",
      " [[9997 1363]\n",
      " [1475 2225]]\n",
      "Error rate:  0.18844621513944226\n",
      "Training time:  17.183485746383667\n",
      "Prediction time:  0.10996699333190918\n"
     ]
    }
   ],
   "source": [
    "# Print the columns in ds_combined_clean\n",
    "print(\"Columns in ds_combined_clean: \", ds_combined_clean.columns.tolist(), \"\\n\")\n",
    "\n",
    "# Print columns selected after removing binned columns and dataset column\n",
    "features_selected = list(set(ds_combined_clean.columns) - set([item + \"-binned\" for item in encoded_columns_binned_list] + ['dataset']))\n",
    "print(\"Columns selected with ds_header_names_encoded: \", features_selected, \"\\n\")\n",
    "\n",
    "# Test the CART decision tree with encoded columns with continuous values\n",
    "print(\"Testing CART decision tree with encoded columns using continuous values\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "# Split the data into train and test, selecting only the encoded columns\n",
    "train_data = ds_combined_clean[ds_combined_clean['dataset'] == 'train'].drop(columns=['dataset'])\n",
    "train_data = train_data[features_selected].drop(columns=['income-num'])\n",
    "train_target = ds_combined_clean[ds_combined_clean['dataset'] == 'train']['income-num']\n",
    "test_data = ds_combined_clean[ds_combined_clean['dataset'] == 'test'].drop(columns=['dataset'])\n",
    "test_data = test_data[features_selected].drop(columns=['income-num'])\n",
    "test_target = ds_combined_clean[ds_combined_clean['dataset'] == 'test']['income-num']\n",
    "\n",
    "# Print the shape of the train and test data, train and test target, and variable type\n",
    "print(\"Shape of train_data: \", train_data.shape)\n",
    "print(\"Shape of train_target: \", train_target.shape)\n",
    "print(\"Shape of test_data: \", test_data.shape)\n",
    "print(\"Shape of test_target: \", test_target.shape)\n",
    "print(\"Variable type of train_data: \", type(train_data))\n",
    "print(\"Variable type of train_target: \", type(train_target))\n",
    "print(\"Variable type of test_data: \", type(test_data))\n",
    "print(\"Variable type of test_target: \", type(test_target), \"\\n\")\n",
    "\n",
    "# Initialize the CART decision tree\n",
    "cart_decision_tree = CARTDecisionTree()\n",
    "\n",
    "# Fit the CART decision tree and measure time taken\n",
    "start_time = time.time()\n",
    "cart_decision_tree.fit(train_data.to_numpy(), train_target.to_numpy())\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Print the CART decision tree\n",
    "# print(\"CART decision tree:\")\n",
    "# cart_decision_tree.print_tree(cart_decision_tree.root, feature_names=train_data.columns.tolist())\n",
    "\n",
    "# Predict the target column and meaure time taken\n",
    "start_time = time.time()\n",
    "y_pred = cart_decision_tree.predict(test_data.to_numpy())\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Print count of predicted values\n",
    "print(\"Count of predicted values:\")\n",
    "print(np.unique(y_pred, return_counts=True), \"\\n\")\n",
    "\n",
    "# Calculate the accuracy, precision, recall, F1 score, and confusion matrix\n",
    "accuracy = cart_decision_tree.accuracy(test_target.to_numpy(), y_pred)\n",
    "precision = cart_decision_tree.precision(test_target.to_numpy(), y_pred)\n",
    "recall = cart_decision_tree.recall(test_target.to_numpy(), y_pred)\n",
    "f1_score = cart_decision_tree.f1_score(test_target.to_numpy(), y_pred)\n",
    "confusion_matrix = cart_decision_tree.confusion_matrix(test_target.to_numpy(), y_pred)\n",
    "\n",
    "# Print the accuracy, precision, recall, F1 score, and confusion matrix\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1_score)\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix)\n",
    "print(\"Error rate: \", 1 - accuracy)\n",
    "\n",
    "# Print the training and prediction time\n",
    "print(\"Training time: \", training_time)\n",
    "print(\"Prediction time: \", prediction_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test algorithm with continuous value and with cross validation k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ds_combined_clean:  ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'dataset', 'occupation-num', 'income-num', 'sex-num', 'race-num', 'workclass-num', 'native-country-num', 'relationship-num', 'age-binned', 'hours-per-week-binned', 'capital-gain-binned', 'capital-loss-binned'] \n",
      "\n",
      "Columns selected with ds_header_names_encoded:  ['workclass-num', 'occupation-num', 'relationship-num', 'race-num', 'capital-gain', 'native-country-num', 'income-num', 'capital-loss', 'education-num', 'hours-per-week', 'age', 'sex-num'] \n",
      "\n",
      "Testing CART decision tree with encoded columns using continuous values and cross validation\n",
      "==============================================\n",
      "Shape of cv_data:  (45222, 11)\n",
      "Shape of cv_target:  (45222,)\n",
      "Variable type of cv_data:  <class 'pandas.core.frame.DataFrame'>\n",
      "Variable type of cv_target:  <class 'pandas.core.series.Series'> \n",
      "\n",
      "Accuracy:  0.8152145068553738\n",
      "Precision:  0.7523067285623006\n",
      "Recall:  0.7473976213117102\n",
      "F1 score:  0.749841917683555\n",
      "Confusion matrix: \n",
      " [[5998.8  803.8]\n",
      " [ 867.4 1374. ]]\n",
      "Error rate:  0.18478549314462622 \n",
      "\n",
      "Combined training time:  99.99959135055542\n",
      "Average training time:  19.999918270111085\n",
      "Prediction time:  0.3212130069732666\n",
      "Average prediction time:  0.06424260139465332\n",
      "Cross validation time:  100.35412883758545\n"
     ]
    }
   ],
   "source": [
    "# Print the columns in ds_combined_clean\n",
    "print(\"Columns in ds_combined_clean: \", ds_combined_clean.columns.tolist(), \"\\n\")\n",
    "\n",
    "# Print columns selected after removing binned columns and dataset column\n",
    "features_selected = list(set(ds_combined_clean.columns) - set([item + \"-binned\" for item in encoded_columns_binned_list] + ['dataset']))\n",
    "print(\"Columns selected with ds_header_names_encoded: \", features_selected, \"\\n\")\n",
    "\n",
    "# Test the CART decision tree with encoded columns with continuous values and cross validation\n",
    "print(\"Testing CART decision tree with encoded columns using continuous values and cross validation\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "# Prepare features and target\n",
    "cv_data = ds_combined_clean[features_selected].drop(columns=['income-num'])\n",
    "cv_target = ds_combined_clean['income-num']\n",
    "\n",
    "# Print the shape of the data and target, and variable type\n",
    "print(\"Shape of cv_data: \", cv_data.shape)\n",
    "print(\"Shape of cv_target: \", cv_target.shape)\n",
    "print(\"Variable type of cv_data: \", type(cv_data))\n",
    "print(\"Variable type of cv_target: \", type(cv_target), \"\\n\")\n",
    "\n",
    "# Initialize the CART decision tree\n",
    "cart_decision_tree = CARTDecisionTree()\n",
    "k = 5\n",
    "\n",
    "# Perform cross validation and measure time taken\n",
    "start_time = time.time()\n",
    "accuracy, precision, recall, f1_score, confusion_matrix, training_time, predict_time = cart_decision_tree.cross_validation(cv_data.to_numpy(), cv_target.to_numpy(), k)\n",
    "cross_validation_time = time.time() - start_time\n",
    "\n",
    "# Print the accuracy, precision, recall, F1 score, and confusion matrix\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1_score)\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix)\n",
    "print(\"Error rate: \", 1 - accuracy, \"\\n\")\n",
    "\n",
    "# Print the training and prediction time\n",
    "print(\"Combined training time: \", training_time)\n",
    "print(\"Average training time: \", training_time/k)\n",
    "print(\"Prediction time: \", predict_time)\n",
    "print(\"Average prediction time: \", predict_time/k)\n",
    "print(\"Cross validation time: \", cross_validation_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Emmanuel, T., Maupong, T., Mpoeleng, D., Semong, T., Mphago, B., & Tabona, O. (2021). A survey on missing data in machine learning. Journal of Big Data, 8(1), 140. https://doi.org/10.1186/s40537-021-00516-9\n",
    "2. Sharma, S. (2021, November 8). Decision Tree and its types. Medium. https://sid-sharma1990.medium.com/decision-tree-and-its-types-76db66644622\n",
    "3. Decision Trees. (n.d.). Retrieved 24 September 2023, from https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/\n",
    "4. Decision Tree. (2017, October 16). GeeksforGeeks. https://www.geeksforgeeks.org/decision-tree/\n",
    "5. K, D. (2023, February 7). Top 5 advantages and disadvantages of Decision Tree Algorithm. Medium. https://dhirajkumarblog.medium.com/top-5-advantages-and-disadvantages-of-decision-tree-algorithm-428ebd199d9a\n",
    "6. How to Deal with Missing Data. (n.d.). CORP-MIDS1 (MDS). Retrieved 23 September 2023, from https://www.mastersindatascience.org/learning/how-to-deal-with-missing-data/\n",
    "7. Severson, K. A., Molaro, M. C., & Braatz, R. D. (2017). Principal Component Analysis of Process Datasets with Missing Values. Processes, 5(3), Article 3. https://doi.org/10.3390/pr5030038\n",
    "8. Acock, A. C. (2005). Working with Missing Values. Journal of Marriage and Family, 67(4), 1012–1028.\n",
    "9. Wang, J. (2022). Research on Income Forecasting based on Machine Learning Methods and the Importance of Features. Proceedings of the International Conference on Information Economy, Data Modeling and Cloud Computing, ICIDC 2022, 17-19 June 2022, Qingdao, China. Proceedings of the International Conference on Information Economy, Data Modeling and Cloud Computing, ICIDC 2022, 17-19 June 2022, Qingdao, China, Qingdao, People’s Republic of China. https://doi.org/10.4108/eai.17-6-2022.2322745\n",
    "10. Chakrabarty, N., & Biswas, S. (2018). A Statistical Approach to Adult Census Income Level Prediction. 2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN), 207–212. https://doi.org/10.1109/ICACCCN.2018.8748528\n",
    "11. Comparative Study of Classifiers in predicting the Income Range of a person from a census data | by Ritvik Khanna | Towards Data Science. (n.d.). Retrieved 23 September 2023, from https://towardsdatascience.com/comparative-study-of-classifiers-in-predicting-the-income-range-of-a-person-from-a-census-data-96ce60ee5a10\n",
    "12. Agarwal, A. (2019, April 20). Logistic Regression classifier on Census Income Data. Medium. https://towardsdatascience.com/logistic-regression-classifier-on-census-income-data-e1dbef0b5738\n",
    "13. Guo, H., Nguyen, H., Vu, D.-A., & Bui, X.-N. (2019). Forecasting mining capital cost for open-pit mining projects based on artificial neural network approach. Resources Policy, 74. https://doi.org/10.1016/j.resourpol.2019.101474\n",
    "14. CART (Classification And Regression Tree) in Machine Learning. (2022, September 23). GeeksforGeeks. https://www.geeksforgeeks.org/cart-classification-and-regression-tree-in-machine-learning/\n",
    "15. deepankar. (2021, April 22). Decision Trees with CART Algorithm. Geek Culture. https://medium.com/geekculture/decision-trees-with-cart-algorithm-\n",
    "16. Sumitkrsharma. (2022, January 21). Understanding Decision Trees CART Algorithm | Machine Learning. Medium. https://sumit-kr-sharma.medium.com/understanding-decision-trees-cart-algorithm-machine-learning-748f0c2249a6\n",
    "17. Boonamnuay, S., Kerdprasop, N., & Kerdprasop, K. (2018). Classification and Regression Tree with Resampling for Classifying Imbalanced Data. International Journal of Machine Learning and Computing, 8, 336–340. https://doi.org/10.18178/ijmlc.2018.8.4.708\n",
    "18. Zhao, L., Lee, S., & Jeong, S.-P. (2021). Decision Tree Application to Classification Problems with Boosting Algorithm. Electronics, 10(16), Article 16. https://doi.org/10.3390/electronics10161903\n",
    "19. Rokach, L., & Maimon, O. (2005). Decision Trees. In Data Mining and Knowledge Discovery Handbook (pp. 165–192). Springer, Boston, MA. https://doi.org/10.1007/0-387-25465-X_9\n",
    "20. Galarnyk, M. (2022, April 27). Understanding Decision Trees for Classification (Python). Medium. https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952\n",
    "21. Dash, S. (2022, November 2). Decision Trees Explained—Entropy, Information Gain, Gini Index, CCP Pruning.. Medium. https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c\n",
    "22. ML | Gini Impurity and Entropy in Decision Tree. (2020, July 14). GeeksforGeeks. https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/\n",
    "23. Valente, J. (2019, October 31). Decision Tree from Scratch in Python. Medium. https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IN6277",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
